{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ RAG Pipeline - End to End (Local Version)\n",
    "\n",
    "This notebook implements a complete **Retrieval Augmented Generation (RAG)** pipeline:\n",
    "\n",
    "1. **Load** a PDF file from local path\n",
    "2. **Chunk** into semantic pieces\n",
    "3. **Embed** using OpenAI embeddings\n",
    "4. **Store** in Qdrant vector database\n",
    "5. **Query** and retrieve relevant context\n",
    "\n",
    "Run each cell sequentially to build your RAG system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Install Dependencies\n",
    "Run this cell first to install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/macbookpro/Documents/Agent-langchain-rag-mcp-tools-boilerplate/venv/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "âœ… All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q pdfplumber openai qdrant-client rank-bm25 numpy python-dotenv sentence-transformers\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Configuration\n",
    "Set up your OpenAI API key and other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API Key found!\n",
      "\n",
      "ðŸ“‹ Configuration:\n",
      "   Embedding model: text-embedding-3-large\n",
      "   Chunk size: 200-800 chars\n",
      "   Qdrant path: ./qdrant_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# =============================================================================\n",
    "# API KEY SETUP\n",
    "# =============================================================================\n",
    "# Option 1: Set directly here\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"\n",
    "\n",
    "# Option 2: Use .env file (recommended) - already loaded above\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸  OPENAI_API_KEY not found!\")\n",
    "    print(\"   Set it in .env file or uncomment Option 1 above\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API Key found!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Paths\n",
    "QDRANT_PATH = \"./qdrant_db\"  # Local storage for Qdrant\n",
    "COLLECTION_NAME = \"rag_collection\"\n",
    "\n",
    "# Embedding settings (OpenAI text-embedding-3-large)\n",
    "EMBED_MODEL = \"text-embedding-3-large\"\n",
    "EMBED_DIMENSION = 3072\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_MIN_SIZE = 200   # Minimum characters per chunk\n",
    "CHUNK_MAX_SIZE = 800   # Maximum characters per chunk\n",
    "\n",
    "# Search settings\n",
    "VECTOR_WEIGHT = 0.7    # Weight for vector (semantic) search\n",
    "BM25_WEIGHT = 0.3      # Weight for BM25 (keyword) search\n",
    "DEFAULT_TOP_K = 5      # Number of results to retrieve\n",
    "\n",
    "print(f\"\\nðŸ“‹ Configuration:\")\n",
    "print(f\"   Embedding model: {EMBED_MODEL}\")\n",
    "print(f\"   Chunk size: {CHUNK_MIN_SIZE}-{CHUNK_MAX_SIZE} chars\")\n",
    "print(f\"   Qdrant path: {QDRANT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Specify PDF File Path\n",
    "Enter the path to your PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found: ./documents/jd1.pdf\n",
      "\n",
      "ðŸ“„ 1 file(s) ready to process\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ“ ENTER YOUR PDF FILE PATH HERE\n",
    "# =============================================================================\n",
    "PDF_FILE_PATH = \"./documents/jd1.pdf\"  \n",
    "\n",
    "# Or specify multiple files as a list:\n",
    "# PDF_FILE_PATHS = [\n",
    "#     \"./documents/file1.pdf\",\n",
    "#     \"./documents/file2.pdf\",\n",
    "# ]\n",
    "\n",
    "# Convert to list for unified processing\n",
    "if isinstance(PDF_FILE_PATH, str):\n",
    "    pdf_files = [PDF_FILE_PATH]\n",
    "else:\n",
    "    pdf_files = PDF_FILE_PATH\n",
    "\n",
    "# Validate files exist\n",
    "valid_files = []\n",
    "for filepath in pdf_files:\n",
    "    if os.path.exists(filepath):\n",
    "        valid_files.append(filepath)\n",
    "        print(f\"âœ… Found: {filepath}\")\n",
    "    else:\n",
    "        print(f\"âŒ Not found: {filepath}\")\n",
    "\n",
    "if not valid_files:\n",
    "    print(\"\\nâš ï¸  No valid PDF files found! Please check the path above.\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“„ {len(valid_files)} file(s) ready to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Document Loader\n",
    "Extract text from PDF files using pdfplumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Loading documents...\n",
      "   âœ“ Loaded: jd1.pdf (29705 chars, 10 pages)\n",
      "\n",
      "âœ… Loaded 1 document(s)!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a loaded document with metadata.\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict\n",
    "    source: str\n",
    "    doc_type: str\n",
    "\n",
    "def load_pdf(filepath: str) -> Document:\n",
    "    \"\"\"Load a single PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            page_count = len(pdf.pages)\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸  Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return Document(\n",
    "        content=text.strip(),\n",
    "        metadata={\n",
    "            \"page_count\": page_count,\n",
    "            \"file_size\": os.path.getsize(filepath),\n",
    "            \"filename\": os.path.basename(filepath)\n",
    "        },\n",
    "        source=filepath,\n",
    "        doc_type=\"pdf\"\n",
    "    )\n",
    "\n",
    "# Load all PDF files\n",
    "print(\"ðŸ“„ Loading documents...\")\n",
    "documents = []\n",
    "\n",
    "for filepath in valid_files:\n",
    "    if filepath.lower().endswith('.pdf'):\n",
    "        doc = load_pdf(filepath)\n",
    "        if doc:\n",
    "            documents.append(doc)\n",
    "            # print(\"  âœ“ Loaded: \", doc.content)\n",
    "            print(f\"   âœ“ Loaded: {doc.metadata['filename']} ({len(doc.content)} chars, {doc.metadata['page_count']} pages)\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} document(s)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Semantic Chunker\n",
    "Split documents into meaningful chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from dataclasses import dataclass\n",
    "# from typing import List, Dict\n",
    "\n",
    "# @dataclass\n",
    "# class Chunk:\n",
    "#     \"\"\"Represents a text chunk with metadata.\"\"\"\n",
    "#     text: str\n",
    "#     metadata: Dict\n",
    "#     chunk_index: int\n",
    "\n",
    "# def clean_text(text: str) -> str:\n",
    "#     \"\"\"Clean and normalize text.\"\"\"\n",
    "#     text = re.sub(r'\\b\\d+:\\d+\\b', '', text)  # Remove timestamps\n",
    "#     text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "#     return text.strip()\n",
    "\n",
    "# def split_into_sentences(text: str) -> List[str]:\n",
    "#     \"\"\"Split text into sentences.\"\"\"\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "#     return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# def merge_small_chunks(chunks: List[str]) -> List[str]:\n",
    "#     \"\"\"Merge chunks that are too small.\"\"\"\n",
    "#     if not chunks:\n",
    "#         return []\n",
    "    \n",
    "#     merged = []\n",
    "#     current = chunks[0]\n",
    "    \n",
    "#     for chunk in chunks[1:]:\n",
    "#         if len(current) < CHUNK_MIN_SIZE:\n",
    "#             current += ' ' + chunk\n",
    "#         else:\n",
    "#             merged.append(current)\n",
    "#             current = chunk\n",
    "    \n",
    "#     merged.append(current)\n",
    "#     return merged\n",
    "\n",
    "# def chunk_document(content: str, source: str) -> List[Chunk]:\n",
    "#     \"\"\"Chunk a document using adaptive strategy.\"\"\"\n",
    "#     # Remove timestamps\n",
    "#     content = re.sub(r'\\b\\d+:\\d+\\b', '', content)\n",
    "    \n",
    "#     # Check if text has proper punctuation\n",
    "#     has_punctuation = any(content.count(p) > 5 for p in ['.', '!', '?'])\n",
    "    \n",
    "#     if has_punctuation:\n",
    "#         # Sentence-based chunking for well-formatted text\n",
    "#         cleaned = clean_text(content)\n",
    "#         sentences = split_into_sentences(cleaned)\n",
    "        \n",
    "#         raw_chunks = []\n",
    "#         current_chunk = []\n",
    "#         current_size = 0\n",
    "        \n",
    "#         for sentence in sentences:\n",
    "#             sentence_len = len(sentence)\n",
    "            \n",
    "#             if current_size + sentence_len > CHUNK_MAX_SIZE and current_chunk:\n",
    "#                 raw_chunks.append(' '.join(current_chunk))\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "            \n",
    "#             current_chunk.append(sentence)\n",
    "#             current_size += sentence_len + 1\n",
    "            \n",
    "#             if current_size >= CHUNK_MAX_SIZE * 0.6 and sentence.endswith(('.', '!', '?')):\n",
    "#                 raw_chunks.append(' '.join(current_chunk))\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "        \n",
    "#         if current_chunk:\n",
    "#             raw_chunks.append(' '.join(current_chunk))\n",
    "\n",
    "#         print(raw_chunks)\n",
    "#     else:\n",
    "#         # Line-based chunking for transcripts\n",
    "#         lines = content.split('\\n')\n",
    "#         lines = [l.strip() for l in lines if l.strip()]\n",
    "        \n",
    "#         raw_chunks = []\n",
    "#         current_chunk = []\n",
    "#         current_size = 0\n",
    "        \n",
    "#         for line in lines:\n",
    "#             line_len = len(line)\n",
    "            \n",
    "#             if current_size + line_len > CHUNK_MAX_SIZE and current_chunk:\n",
    "#                 chunk_text = ' '.join(current_chunk)\n",
    "#                 chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()\n",
    "#                 raw_chunks.append(chunk_text)\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "            \n",
    "#             current_chunk.append(line)\n",
    "#             current_size += line_len\n",
    "            \n",
    "#             if current_size >= CHUNK_MAX_SIZE * 0.6:\n",
    "#                 chunk_text = ' '.join(current_chunk)\n",
    "#                 chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()\n",
    "#                 raw_chunks.append(chunk_text)\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "        \n",
    "#         if current_chunk:\n",
    "#             chunk_text = ' '.join(current_chunk)\n",
    "#             chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()\n",
    "#             raw_chunks.append(chunk_text)\n",
    "    \n",
    "#     # Merge small chunks\n",
    "#     final_chunks = merge_small_chunks(raw_chunks)\n",
    "    \n",
    "#     # Create Chunk objects\n",
    "#     chunks = []\n",
    "#     for i, text in enumerate(final_chunks):\n",
    "#         if text and len(text) > 50:\n",
    "#             chunks.append(Chunk(\n",
    "#                 text=text,\n",
    "#                 metadata={\n",
    "#                     \"source\": source,\n",
    "#                     \"chunk_index\": i,\n",
    "#                     \"total_chunks\": len(final_chunks),\n",
    "#                     \"char_count\": len(text)\n",
    "#                 },\n",
    "#                 chunk_index=i\n",
    "#             ))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# # Chunk all documents\n",
    "# print(\"âœ‚ï¸  Chunking documents...\")\n",
    "# all_chunks = []\n",
    "\n",
    "# for doc in documents:\n",
    "#     chunks = chunk_document(doc.content, doc.source)\n",
    "#     all_chunks.extend(chunks)\n",
    "#     # print(f\"   âœ“ {doc.metadata['filename']}: {len(chunks)} chunks\")\n",
    "\n",
    "# print(f\"\\nâœ… Created {len(all_chunks)} total chunks!\")\n",
    "\n",
    "# # Preview first chunk\n",
    "# if all_chunks:\n",
    "#     print(f\"\\nðŸ“ Sample chunk (first 200 chars):\")\n",
    "#     # print(f\"   '{all_chunks[0]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ CHUNKING KING is ready!\n",
      "   âœ… Semantic chunking (embedding-based)\n",
      "   âœ… Speaker-based (for Q&A)\n",
      "   âœ… Sentence-based\n",
      "   âœ… Recursive\n",
      "   âœ… Fixed with overlap\n",
      "   âœ… Auto-detection\n",
      "\n",
      "ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥\n",
      "CHUNKING KING - Processing Documents\n",
      "ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥\n",
      "\n",
      "============================================================\n",
      "âœ‚ï¸ ADVANCED CHUNKING: ./documents/jd1.pdf\n",
      "============================================================\n",
      "ðŸ“ Auto-detected: Speaker-based (Q&A format)\n",
      "ðŸ”§ Using strategy: speaker\n",
      "   âœ“ Created 110 raw chunks\n",
      "   âœ“ Merged to 96 chunks\n",
      "   âœ“ Added 100 char overlap\n",
      "   âœ“ Final: 96 chunks (avg 407 chars)\n",
      "============================================================\n",
      "   âœ“ jd1.pdf: 96 chunks\n",
      "\n",
      "============================================================\n",
      "âœ… TOTAL: 96 chunks from 1 documents\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Sample chunk:\n",
      "   Strategy: speaker\n",
      "   Size: 284 chars\n",
      "   Text: ' J. Krishnamurti's Fifth Public Discussion in London, 1965----- Krishnamurti: If I may, Iâ€™d like to go on with what we were talking the other day; that is, if youâ€™re not bored with what we have talked...'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ðŸ”¥ ADVANCED CHUNKING ENGINE - THE CHUNKING KING\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a text chunk with rich metadata.\"\"\"\n",
    "    text: str\n",
    "    metadata: Dict\n",
    "    chunk_index: int\n",
    "    parent_text: Optional[str] = None  # For parent-child chunking\n",
    "    neighbors: List[int] = field(default_factory=list)  # Adjacent chunk indices\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'\\b\\d+:\\d+\\b', '', text)  # Remove timestamps\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    v1, v2 = np.array(vec1), np.array(vec2)\n",
    "    norm1, norm2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(v1, v2) / (norm1 * norm2))\n",
    "\n",
    "# =============================================================================\n",
    "# CHUNKING STRATEGIES\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_fixed_size(text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 1: Fixed-size chunking with overlap.\n",
    "    Simple but fast. Good for uniform content.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at sentence boundary\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            if last_period > chunk_size * 0.5:\n",
    "                end = start + last_period + 1\n",
    "                chunk = text[start:end]\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap  # Overlap for context continuity\n",
    "    \n",
    "    return [c for c in chunks if len(c) > 50]\n",
    "\n",
    "def chunk_sentence_based(text: str, min_size: int = 200, max_size: int = 800) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Sentence-based chunking.\n",
    "    Respects sentence boundaries. Good for essays/books.\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_len = len(sentence)\n",
    "        \n",
    "        # If adding this sentence exceeds max, save current chunk\n",
    "        if current_size + sentence_len > max_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_len + 1\n",
    "        \n",
    "        # If chunk is large enough and ends with punctuation, save it\n",
    "        if current_size >= min_size and sentence.endswith(('.', '!', '?')):\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return [c for c in chunks if len(c) > 50]\n",
    "\n",
    "def chunk_by_speaker(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 3: Speaker-based chunking for dialogues.\n",
    "    Keeps each speaker's complete thought together.\n",
    "    Best for Krishnamurti Q&A format!\n",
    "    \"\"\"\n",
    "    # Pattern to detect speaker turns\n",
    "    speaker_pattern = r'(Questioner\\s*:|Krishnamurti\\s*:|Q\\s*:|K\\s*:)'\n",
    "    \n",
    "    # Split by speaker\n",
    "    parts = re.split(speaker_pattern, text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_speaker = \"\"\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(speaker_pattern, part):\n",
    "            current_speaker = part\n",
    "        elif part.strip():\n",
    "            # Combine speaker label with their content\n",
    "            chunk = f\"{current_speaker} {part.strip()}\"\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return [c for c in chunks if len(c) > 50]\n",
    "\n",
    "def chunk_by_paragraph(text: str, max_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 4: Paragraph-based chunking.\n",
    "    Good for structured documents with clear paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_len = len(para)\n",
    "        \n",
    "        if current_size + para_len > max_size and current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        \n",
    "        current_chunk.append(para)\n",
    "        current_size += para_len\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n\\n'.join(current_chunk))\n",
    "    \n",
    "    return [c for c in chunks if len(c) > 50]\n",
    "\n",
    "def chunk_semantic(text: str, threshold: float = 0.75, min_size: int = 200, max_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 5: Semantic chunking using embeddings.\n",
    "    Splits when topic changes significantly.\n",
    "    Most intelligent but slowest!\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    \n",
    "    if len(sentences) < 2:\n",
    "        return [text] if len(text) > 50 else []\n",
    "    \n",
    "    print(\"   ðŸ§  Computing sentence embeddings for semantic chunking...\")\n",
    "    \n",
    "    # Embed sentences in batches\n",
    "    embeddings = []\n",
    "    batch_size = 20\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        batch_embeddings = [embed_text(s) for s in batch]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    # Find split points where similarity drops\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_size = len(sentences[0])\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        similarity = cosine_similarity(embeddings[i-1], embeddings[i])\n",
    "        sentence_len = len(sentences[i])\n",
    "        \n",
    "        # Split if: topic changed AND chunk is big enough\n",
    "        should_split = (\n",
    "            similarity < threshold and \n",
    "            current_size >= min_size\n",
    "        ) or (current_size + sentence_len > max_size)\n",
    "        \n",
    "        if should_split and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_size = sentence_len\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_size += sentence_len\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return [c for c in chunks if len(c) > 50]\n",
    "\n",
    "def chunk_recursive(text: str, max_size: int = 800) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 6: Recursive chunking.\n",
    "    Tries multiple delimiters in order: \\n\\n â†’ \\n â†’ . â†’ space\n",
    "    Good for mixed content.\n",
    "    \"\"\"\n",
    "    separators = ['\\n\\n', '\\n', '. ', ' ']\n",
    "    \n",
    "    def split_recursive(text: str, separators: List[str]) -> List[str]:\n",
    "        if not separators or len(text) <= max_size:\n",
    "            return [text] if text.strip() else []\n",
    "        \n",
    "        separator = separators[0]\n",
    "        remaining_separators = separators[1:]\n",
    "        \n",
    "        parts = text.split(separator)\n",
    "        \n",
    "        chunks = []\n",
    "        current = []\n",
    "        current_len = 0\n",
    "        \n",
    "        for part in parts:\n",
    "            part_len = len(part) + len(separator)\n",
    "            \n",
    "            if current_len + part_len > max_size and current:\n",
    "                chunk_text = separator.join(current)\n",
    "                # If still too big, recurse with next separator\n",
    "                if len(chunk_text) > max_size:\n",
    "                    chunks.extend(split_recursive(chunk_text, remaining_separators))\n",
    "                else:\n",
    "                    chunks.append(chunk_text)\n",
    "                current = []\n",
    "                current_len = 0\n",
    "            \n",
    "            current.append(part)\n",
    "            current_len += part_len\n",
    "        \n",
    "        if current:\n",
    "            chunk_text = separator.join(current)\n",
    "            if len(chunk_text) > max_size:\n",
    "                chunks.extend(split_recursive(chunk_text, remaining_separators))\n",
    "            else:\n",
    "                chunks.append(chunk_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    return [c.strip() for c in split_recursive(text, separators) if len(c.strip()) > 50]\n",
    "\n",
    "# =============================================================================\n",
    "# OVERLAP ADDING\n",
    "# =============================================================================\n",
    "\n",
    "def add_overlap(chunks: List[str], overlap_chars: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Add overlap between consecutive chunks.\n",
    "    Ensures context is preserved at boundaries.\n",
    "    \"\"\"\n",
    "    if len(chunks) < 2:\n",
    "        return chunks\n",
    "    \n",
    "    overlapped = [chunks[0]]\n",
    "    \n",
    "    for i in range(1, len(chunks)):\n",
    "        # Get last N chars from previous chunk\n",
    "        prev_end = chunks[i-1][-overlap_chars:] if len(chunks[i-1]) > overlap_chars else chunks[i-1]\n",
    "        # Prepend to current chunk\n",
    "        overlapped.append(f\"{prev_end}... {chunks[i]}\")\n",
    "    \n",
    "    return overlapped\n",
    "\n",
    "# =============================================================================\n",
    "# PARENT-CHILD CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_parent_child(text: str, parent_size: int = 2000, child_size: int = 500) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Strategy 7: Parent-Child chunking.\n",
    "    Creates large parent chunks and small child chunks.\n",
    "    Search on children, return parent for context!\n",
    "    \"\"\"\n",
    "    # Create parent chunks\n",
    "    parents = chunk_fixed_size(text, chunk_size=parent_size, overlap=200)\n",
    "    \n",
    "    # Create child chunks from each parent\n",
    "    all_children = []\n",
    "    child_to_parent = {}\n",
    "    \n",
    "    for parent_idx, parent in enumerate(parents):\n",
    "        children = chunk_sentence_based(parent, min_size=200, max_size=child_size)\n",
    "        for child in children:\n",
    "            child_idx = len(all_children)\n",
    "            all_children.append(child)\n",
    "            child_to_parent[child_idx] = parent_idx\n",
    "    \n",
    "    return parents, all_children, child_to_parent\n",
    "\n",
    "# =============================================================================\n",
    "# MERGE SMALL CHUNKS\n",
    "# =============================================================================\n",
    "\n",
    "def merge_small_chunks(chunks: List[str], min_size: int = 200) -> List[str]:\n",
    "    \"\"\"Merge chunks that are too small with neighbors.\"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    merged = []\n",
    "    current = chunks[0]\n",
    "    \n",
    "    for chunk in chunks[1:]:\n",
    "        if len(current) < min_size:\n",
    "            current += ' ' + chunk\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            current = chunk\n",
    "    \n",
    "    merged.append(current)\n",
    "    return merged\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ”¥ THE CHUNKING KING - MAIN FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_chunk_document(\n",
    "    content: str, \n",
    "    source: str,\n",
    "    strategy: str = \"auto\",  # auto, semantic, speaker, sentence, recursive, fixed\n",
    "    use_overlap: bool = True,\n",
    "    overlap_chars: int = 100,\n",
    "    min_chunk_size: int = 200,\n",
    "    max_chunk_size: int = 800,\n",
    "    semantic_threshold: float = 0.75,\n",
    "    verbose: bool = True\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    ðŸ”¥ Advanced Document Chunking with multiple strategies!\n",
    "    \n",
    "    Strategies:\n",
    "    - auto: Automatically selects best strategy based on content\n",
    "    - semantic: Uses embeddings to detect topic changes\n",
    "    - speaker: Splits by speaker (for Q&A/dialogues)\n",
    "    - sentence: Respects sentence boundaries\n",
    "    - recursive: Tries multiple delimiters\n",
    "    - fixed: Fixed-size with overlap\n",
    "    \n",
    "    Features:\n",
    "    - Smart strategy selection\n",
    "    - Optional overlap for context\n",
    "    - Metadata with neighbors\n",
    "    - Quality filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ‚ï¸ ADVANCED CHUNKING: {source}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clean the content\n",
    "    content = re.sub(r'\\b\\d+:\\d+\\b', '', content)  # Remove timestamps\n",
    "    cleaned = clean_text(content)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AUTO-DETECT BEST STRATEGY\n",
    "    # =========================================================================\n",
    "    if strategy == \"auto\":\n",
    "        # Check for speaker patterns (Q&A format)\n",
    "        has_speakers = bool(re.search(r'(Questioner\\s*:|Krishnamurti\\s*:|Q\\s*:|K\\s*:)', content))\n",
    "        \n",
    "        # Check for punctuation (well-formatted text)\n",
    "        has_punctuation = sum(content.count(p) for p in ['.', '!', '?']) > 10\n",
    "        \n",
    "        # Check for paragraphs\n",
    "        has_paragraphs = content.count('\\n\\n') > 5\n",
    "        \n",
    "        if has_speakers:\n",
    "            strategy = \"speaker\"\n",
    "            if verbose:\n",
    "                print(f\"ðŸ“ Auto-detected: Speaker-based (Q&A format)\")\n",
    "        elif has_punctuation and len(cleaned) > 5000:\n",
    "            strategy = \"semantic\"\n",
    "            if verbose:\n",
    "                print(f\"ðŸ“ Auto-detected: Semantic (long well-formatted text)\")\n",
    "        elif has_paragraphs:\n",
    "            strategy = \"recursive\"\n",
    "            if verbose:\n",
    "                print(f\"ðŸ“ Auto-detected: Recursive (structured paragraphs)\")\n",
    "        else:\n",
    "            strategy = \"sentence\"\n",
    "            if verbose:\n",
    "                print(f\"ðŸ“ Auto-detected: Sentence-based (default)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # APPLY SELECTED STRATEGY\n",
    "    # =========================================================================\n",
    "    if verbose:\n",
    "        print(f\"ðŸ”§ Using strategy: {strategy}\")\n",
    "    \n",
    "    if strategy == \"semantic\":\n",
    "        raw_chunks = chunk_semantic(cleaned, threshold=semantic_threshold, \n",
    "                                    min_size=min_chunk_size, max_size=max_chunk_size)\n",
    "    elif strategy == \"speaker\":\n",
    "        raw_chunks = chunk_by_speaker(content)  # Use original to preserve speaker labels\n",
    "        # Further split if too long\n",
    "        split_chunks = []\n",
    "        for chunk in raw_chunks:\n",
    "            if len(chunk) > max_chunk_size:\n",
    "                split_chunks.extend(chunk_sentence_based(chunk, min_chunk_size, max_chunk_size))\n",
    "            else:\n",
    "                split_chunks.append(chunk)\n",
    "        raw_chunks = split_chunks\n",
    "    elif strategy == \"sentence\":\n",
    "        raw_chunks = chunk_sentence_based(cleaned, min_chunk_size, max_chunk_size)\n",
    "    elif strategy == \"recursive\":\n",
    "        raw_chunks = chunk_recursive(cleaned, max_chunk_size)\n",
    "    elif strategy == \"fixed\":\n",
    "        raw_chunks = chunk_fixed_size(cleaned, max_chunk_size, overlap_chars)\n",
    "    elif strategy == \"paragraph\":\n",
    "        raw_chunks = chunk_by_paragraph(cleaned, max_chunk_size)\n",
    "    else:\n",
    "        raw_chunks = chunk_sentence_based(cleaned, min_chunk_size, max_chunk_size)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   âœ“ Created {len(raw_chunks)} raw chunks\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MERGE SMALL CHUNKS\n",
    "    # =========================================================================\n",
    "    merged_chunks = merge_small_chunks(raw_chunks, min_chunk_size)\n",
    "    \n",
    "    if verbose and len(merged_chunks) != len(raw_chunks):\n",
    "        print(f\"   âœ“ Merged to {len(merged_chunks)} chunks\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ADD OVERLAP (Optional)\n",
    "    # =========================================================================\n",
    "    if use_overlap and strategy != \"fixed\":  # Fixed already has overlap\n",
    "        final_texts = add_overlap(merged_chunks, overlap_chars)\n",
    "        if verbose:\n",
    "            print(f\"   âœ“ Added {overlap_chars} char overlap\")\n",
    "    else:\n",
    "        final_texts = merged_chunks\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CREATE CHUNK OBJECTS WITH METADATA\n",
    "    # =========================================================================\n",
    "    chunks = []\n",
    "    for i, text in enumerate(final_texts):\n",
    "        if text and len(text) > 50:\n",
    "            # Find neighbors\n",
    "            neighbors = []\n",
    "            if i > 0:\n",
    "                neighbors.append(i - 1)\n",
    "            if i < len(final_texts) - 1:\n",
    "                neighbors.append(i + 1)\n",
    "            \n",
    "            chunks.append(Chunk(\n",
    "                text=text,\n",
    "                metadata={\n",
    "                    \"source\": source,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(final_texts),\n",
    "                    \"char_count\": len(text),\n",
    "                    \"strategy\": strategy,\n",
    "                    \"has_overlap\": use_overlap\n",
    "                },\n",
    "                chunk_index=i,\n",
    "                neighbors=neighbors\n",
    "            ))\n",
    "    \n",
    "    if verbose:\n",
    "        avg_size = sum(len(c.text) for c in chunks) / len(chunks) if chunks else 0\n",
    "        print(f\"   âœ“ Final: {len(chunks)} chunks (avg {avg_size:.0f} chars)\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH DOCUMENT PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_all_documents(\n",
    "    documents: List,\n",
    "    strategy: str = \"auto\",\n",
    "    use_overlap: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"Process multiple documents with advanced chunking.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"ðŸ”¥\" * 20)\n",
    "    print(\"CHUNKING KING - Processing Documents\")\n",
    "    print(\"ðŸ”¥\" * 20)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = advanced_chunk_document(\n",
    "            content=doc.content,\n",
    "            source=doc.source,\n",
    "            strategy=strategy,\n",
    "            use_overlap=use_overlap,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"   âœ“ {doc.metadata.get('filename', 'Unknown')}: {len(chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… TOTAL: {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# =============================================================================\n",
    "# RUN CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nðŸ”¥ CHUNKING KING is ready!\")\n",
    "print(\"   âœ… Semantic chunking (embedding-based)\")\n",
    "print(\"   âœ… Speaker-based (for Q&A)\")\n",
    "print(\"   âœ… Sentence-based\")\n",
    "print(\"   âœ… Recursive\")\n",
    "print(\"   âœ… Fixed with overlap\")\n",
    "print(\"   âœ… Auto-detection\")\n",
    "\n",
    "# Chunk documents\n",
    "all_chunks = chunk_all_documents(documents, strategy=\"auto\", use_overlap=True)\n",
    "\n",
    "# Show sample\n",
    "if all_chunks:\n",
    "    print(f\"\\nðŸ“ Sample chunk:\")\n",
    "    print(f\"   Strategy: {all_chunks[0].metadata.get('strategy')}\")\n",
    "    print(f\"   Size: {len(all_chunks[0].text)} chars\")\n",
    "    print(f\"   Text: '{all_chunks[0].text[:200]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Generate Embeddings\n",
    "Convert text chunks to vector embeddings using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generating embeddings...\n",
      "   âœ“ Embedded 50/96 chunks\n",
      "   âœ“ Embedded 96/96 chunks\n",
      "\n",
      "âœ… Generated 96 embeddings!\n",
      "   Embedding dimension: 3072\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"Embed a single text string.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size: int = 50) -> List[List[float]]:\n",
    "    \"\"\"Embed multiple texts with batching.\"\"\"\n",
    "    all_embeddings = []\n",
    "    total = len(texts)\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        response = client.embeddings.create(\n",
    "            model=EMBED_MODEL,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        processed = min(i + batch_size, total)\n",
    "        print(f\"   âœ“ Embedded {processed}/{total} chunks\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"ðŸ§  Generating embeddings...\")\n",
    "texts = [chunk.text for chunk in all_chunks]\n",
    "embeddings = embed_texts(texts)\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(embeddings)} embeddings!\")\n",
    "print(f\"   Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Store in Qdrant Vector Database\n",
    "Save embeddings to local Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaned up existing database at: ./qdrant_db\n",
      "ðŸ“¦ Created local Qdrant at: ./qdrant_db\n",
      "   ðŸ—‘ï¸  Deleted existing collection: rag_collection\n",
      "   âœ“ Created collection: rag_collection\n",
      "\n",
      "âœ… Stored 96 vectors in Qdrant!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, \n",
    "    Distance, \n",
    "    PointStruct,\n",
    "    ScalarQuantizationConfig,\n",
    "    ScalarType\n",
    ")\n",
    "# ðŸ§¹ Clean up any existing database (prevents lock errors on re-run)\n",
    "if os.path.exists(QDRANT_PATH):\n",
    "    shutil.rmtree(QDRANT_PATH)\n",
    "    print(f\"ðŸ§¹ Cleaned up existing database at: {QDRANT_PATH}\")\n",
    "# Create local Qdrant client (file-based, no server needed)\n",
    "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
    "qdrant_client = QdrantClient(path=QDRANT_PATH)\n",
    "print(f\"ðŸ“¦ Created local Qdrant at: {QDRANT_PATH}\")\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    qdrant_client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"   ðŸ—‘ï¸  Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=EMBED_DIMENSION,\n",
    "        distance=Distance.COSINE\n",
    "    ),\n",
    "    quantization_config=ScalarQuantizationConfig(\n",
    "        type=ScalarType.INT8,\n",
    "        always_ram=True\n",
    "    )\n",
    ")\n",
    "print(f\"   âœ“ Created collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# Prepare points\n",
    "points = []\n",
    "for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):\n",
    "    points.append(PointStruct(\n",
    "        id=i,\n",
    "        vector=embedding,\n",
    "        payload={\n",
    "            \"text\": chunk.text,\n",
    "            \"source\": chunk.metadata.get(\"source\", \"\"),\n",
    "            \"chunk_index\": chunk.chunk_index,\n",
    "            \"metadata\": chunk.metadata\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Upsert points in batches\n",
    "batch_size = 100\n",
    "for i in range(0, len(points), batch_size):\n",
    "    batch = points[i:i + batch_size]\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=batch\n",
    "    )\n",
    "\n",
    "# Get collection info\n",
    "info = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "print(f\"\\nâœ… Stored {info.points_count} vectors in Qdrant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Build BM25 Index (Keyword Search)\n",
    "Create a BM25 index for hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Building BM25 index...\n",
      "\n",
      "âœ… BM25 index built with 96 documents!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization for BM25.\"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return [t for t in tokens if len(t) > 2]\n",
    "\n",
    "# Build BM25 index\n",
    "print(\"ðŸ“š Building BM25 index...\")\n",
    "\n",
    "corpus_texts = [chunk.text for chunk in all_chunks]\n",
    "tokenized_corpus = [tokenize(text) for text in corpus_texts]\n",
    "bm25_index = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Save index and corpus\n",
    "BM25_INDEX_PATH = os.path.join(QDRANT_PATH, \"bm25_index.pkl\")\n",
    "BM25_CORPUS_PATH = os.path.join(QDRANT_PATH, \"bm25_corpus.json\")\n",
    "\n",
    "with open(BM25_INDEX_PATH, 'wb') as f:\n",
    "    pickle.dump(bm25_index, f)\n",
    "\n",
    "with open(BM25_CORPUS_PATH, 'w') as f:\n",
    "    json.dump(corpus_texts, f)\n",
    "\n",
    "print(f\"\\nâœ… BM25 index built with {len(corpus_texts)} documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Query Engine (Hybrid Search)\n",
    "Define functions for searching and retrieving context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# def normalize_scores(scores: List[float]) -> List[float]:\n",
    "#     \"\"\"Normalize scores to 0-1 range.\"\"\"\n",
    "#     if not scores:\n",
    "#         return []\n",
    "#     min_score = min(scores)\n",
    "#     max_score = max(scores)\n",
    "#     if max_score == min_score:\n",
    "#         return [1.0] * len(scores)\n",
    "#     return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "# def search_vectors(query_embedding: List[float], limit: int = 5) -> List[Dict]:\n",
    "#     \"\"\"Search vectors in Qdrant.\"\"\"\n",
    "#     results = qdrant_client.query_points(\n",
    "#         collection_name=COLLECTION_NAME,\n",
    "#         query=query_embedding,\n",
    "#         limit=limit,\n",
    "#     )\n",
    "    \n",
    "#     formatted = []\n",
    "#     for point in results.points:\n",
    "#         formatted.append({\n",
    "#             \"text\": point.payload.get(\"text\", \"\"),\n",
    "#             \"score\": point.score,\n",
    "#             \"source\": point.payload.get(\"source\", \"\"),\n",
    "#             \"metadata\": point.payload.get(\"metadata\", {}),\n",
    "#         })\n",
    "#     return formatted\n",
    "\n",
    "# def search_bm25(query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "#     \"\"\"Search using BM25.\"\"\"\n",
    "#     query_tokens = tokenize(query)\n",
    "#     if not query_tokens:\n",
    "#         return []\n",
    "    \n",
    "#     scores = bm25_index.get_scores(query_tokens)\n",
    "#     top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "#     results = []\n",
    "#     for idx in top_indices:\n",
    "#         if scores[idx] > 0:\n",
    "#             results.append((int(idx), float(scores[idx])))\n",
    "#     return results\n",
    "\n",
    "# def hybrid_search(query: str, top_k: int = 5) -> List[Dict]:\n",
    "#     \"\"\"Perform hybrid search combining vector and BM25.\"\"\"\n",
    "#     print(f\"\\nðŸ” Searching for: '{query}'\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     # Vector search\n",
    "#     query_embedding = embed_text(query)\n",
    "#     vector_results = search_vectors(query_embedding, limit=top_k * 2)\n",
    "#     print(f\"   Vector search: {len(vector_results)} results\")\n",
    "    \n",
    "#     # BM25 search\n",
    "#     bm25_results = search_bm25(query, top_k=top_k * 2)\n",
    "#     print(f\"   BM25 search: {len(bm25_results)} results\")\n",
    "    \n",
    "#     # Combine results\n",
    "#     text_scores = {}\n",
    "#     text_metadata = {}\n",
    "    \n",
    "#     # Process vector results\n",
    "#     vector_scores_list = [r[\"score\"] for r in vector_results]\n",
    "#     normalized_vector = normalize_scores(vector_scores_list)\n",
    "    \n",
    "#     for i, result in enumerate(vector_results):\n",
    "#         text = result[\"text\"]\n",
    "#         score = normalized_vector[i] if normalized_vector else 0\n",
    "#         text_scores[text] = {\"vector\": score, \"bm25\": 0, \"combined\": 0}\n",
    "#         text_metadata[text] = {\n",
    "#             \"source\": result.get(\"source\", \"\"),\n",
    "#             \"metadata\": result.get(\"metadata\", {})\n",
    "#         }\n",
    "    \n",
    "#     # Process BM25 results\n",
    "#     bm25_scores_raw = [score for _, score in bm25_results]\n",
    "#     normalized_bm25 = normalize_scores(bm25_scores_raw)\n",
    "    \n",
    "#     for i, (idx, _) in enumerate(bm25_results):\n",
    "#         text = corpus_texts[idx]\n",
    "#         score = normalized_bm25[i] if normalized_bm25 else 0\n",
    "#         if text in text_scores:\n",
    "#             text_scores[text][\"bm25\"] = score\n",
    "#         else:\n",
    "#             text_scores[text] = {\"vector\": 0, \"bm25\": score, \"combined\": 0}\n",
    "#             text_metadata[text] = {\"source\": \"\", \"metadata\": {}}\n",
    "    \n",
    "#     # Calculate combined scores\n",
    "#     for text in text_scores:\n",
    "#         vs = text_scores[text][\"vector\"]\n",
    "#         bs = text_scores[text][\"bm25\"]\n",
    "#         text_scores[text][\"combined\"] = (vs * VECTOR_WEIGHT) + (bs * BM25_WEIGHT)\n",
    "    \n",
    "#     # Sort and format results\n",
    "#     sorted_texts = sorted(\n",
    "#         text_scores.keys(),\n",
    "#         key=lambda t: text_scores[t][\"combined\"],\n",
    "#         reverse=True\n",
    "#     )\n",
    "    \n",
    "#     results = []\n",
    "#     for text in sorted_texts[:top_k]:\n",
    "#         results.append({\n",
    "#             \"text\": text,\n",
    "#             \"score\": text_scores[text][\"combined\"],\n",
    "#             **text_metadata.get(text, {})\n",
    "#         })\n",
    "    \n",
    "#     print(f\"   Combined: {len(results)} results\")\n",
    "#     print(\"-\" * 50)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def get_context(question: str, k: int = 3) -> str:\n",
    "#     \"\"\"Get formatted context for RAG.\"\"\"\n",
    "#     results = hybrid_search(question, top_k=k)\n",
    "    \n",
    "#     if not results:\n",
    "#         return \"No relevant context found.\"\n",
    "    \n",
    "#     context_parts = []\n",
    "#     for i, result in enumerate(results, 1):\n",
    "#         source = os.path.basename(result.get(\"source\", \"Unknown\"))\n",
    "#         text = result.get(\"text\", \"\")\n",
    "#         score = result.get(\"score\", 0)\n",
    "#         context_parts.append(f\"[Source {i}: {source} | Score: {score:.2f}]\\n{text}\")\n",
    "    \n",
    "#     return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "# print(\"âœ… Query engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225d6519f1cb4189a12f2579319f98f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reranker loaded!\n",
      "\n",
      "ðŸ”¥ SEARCH KING is ready!\n",
      "   âœ… Query expansion: ON\n",
      "   âœ… HyDE search: ON\n",
      "   âœ… Hybrid (Vector + BM25): ON\n",
      "   âœ… Reciprocal Rank Fusion: ON\n",
      "   âœ… Neighbor chunks: ON\n",
      "   âœ… Reranking: ON\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ðŸ”¥ ADVANCED HYBRID SEARCH ENGINE - THE SEARCH KING\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Try to import reranker (optional)\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    HAS_RERANKER = True\n",
    "    print(\"âœ… Reranker loaded!\")\n",
    "except:\n",
    "    HAS_RERANKER = False\n",
    "    print(\"âš ï¸ Reranker not available (install sentence-transformers for better results)\")\n",
    "\n",
    "# =============================================================================\n",
    "# CORE UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_scores(scores: List[float]) -> List[float]:\n",
    "    \"\"\"Normalize scores to 0-1 range.\"\"\"\n",
    "    if not scores:\n",
    "        return []\n",
    "    min_score, max_score = min(scores), max(scores)\n",
    "    if max_score == min_score:\n",
    "        return [1.0] * len(scores)\n",
    "    return [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    v1, v2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# =============================================================================\n",
    "# QUERY ENHANCEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def expand_query_simple(query: str) -> List[str]:\n",
    "    \"\"\"Simple query expansion using common patterns.\"\"\"\n",
    "    expansions = [query]\n",
    "    \n",
    "    # Add variations\n",
    "    if \"what is\" in query.lower():\n",
    "        expansions.append(query.lower().replace(\"what is\", \"explain\"))\n",
    "        expansions.append(query.lower().replace(\"what is\", \"describe\"))\n",
    "    \n",
    "    if \"how\" in query.lower():\n",
    "        expansions.append(query.lower().replace(\"how\", \"what is the way\"))\n",
    "    \n",
    "    # Add keywords\n",
    "    keywords = [\"Krishnamurti\", \"meditation\", \"awareness\", \"thought\", \"observer\"]\n",
    "    for kw in keywords:\n",
    "        if kw.lower() in query.lower():\n",
    "            expansions.append(kw)\n",
    "    \n",
    "    return list(set(expansions))\n",
    "\n",
    "def hyde_search(query: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    HyDE: Hypothetical Document Embeddings\n",
    "    Instead of embedding the question, embed a hypothetical answer.\n",
    "    \"\"\"\n",
    "    # Create a hypothetical answer (simple version without LLM)\n",
    "    hypothetical = f\"\"\"\n",
    "    Krishnamurti addresses this question about {query.lower().replace('?', '')}.\n",
    "    He speaks about the nature of awareness and the observation of the mind.\n",
    "    The key insight is that true understanding comes not from analysis or method,\n",
    "    but from direct perception without the interference of thought.\n",
    "    \"\"\"\n",
    "    return embed_text(hypothetical)\n",
    "\n",
    "# =============================================================================\n",
    "# SEARCH FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def search_vectors(query_embedding: List[float], limit: int = 10) -> List[Dict]:\n",
    "    \"\"\"Search vectors in Qdrant.\"\"\"\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_embedding,\n",
    "        limit=limit,\n",
    "    )\n",
    "    \n",
    "    formatted = []\n",
    "    for point in results.points:\n",
    "        formatted.append({\n",
    "            \"text\": point.payload.get(\"text\", \"\"),\n",
    "            \"score\": point.score,\n",
    "            \"source\": point.payload.get(\"source\", \"\"),\n",
    "            \"metadata\": point.payload.get(\"metadata\", {}),\n",
    "            \"chunk_index\": point.payload.get(\"metadata\", {}).get(\"chunk_index\", 0)\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "def search_bm25(query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Search using BM25.\"\"\"\n",
    "    query_tokens = tokenize(query)\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    scores = bm25_index.get_scores(query_tokens)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if scores[idx] > 0:\n",
    "            results.append((int(idx), float(scores[idx])))\n",
    "    return results\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED FUSION & RANKING\n",
    "# =============================================================================\n",
    "\n",
    "def reciprocal_rank_fusion(results_lists: List[List[Dict]], k: int = 60) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion - combines multiple result lists.\n",
    "    Better than simple score averaging!\n",
    "    \"\"\"\n",
    "    fused_scores = defaultdict(float)\n",
    "    \n",
    "    for results in results_lists:\n",
    "        for rank, doc in enumerate(results):\n",
    "            doc_id = doc[\"text\"]\n",
    "            fused_scores[doc_id] += 1.0 / (k + rank + 1)\n",
    "    \n",
    "    return dict(fused_scores)\n",
    "\n",
    "def rerank_results(query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Rerank results using cross-encoder (if available).\"\"\"\n",
    "    if not HAS_RERANKER or not results:\n",
    "        return results[:top_k]\n",
    "    \n",
    "    pairs = [[query, r[\"text\"]] for r in results]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Combine with original scores (60% rerank, 40% original)\n",
    "    for i, result in enumerate(results):\n",
    "        result[\"rerank_score\"] = float(scores[i])\n",
    "        result[\"final_score\"] = 0.6 * float(scores[i]) + 0.4 * result.get(\"score\", 0)\n",
    "    \n",
    "    ranked = sorted(results, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "def get_neighbor_chunks(chunk_indices: List[int], window: int = 1) -> List[int]:\n",
    "    \"\"\"Get neighboring chunk indices for context.\"\"\"\n",
    "    neighbors = set()\n",
    "    for idx in chunk_indices:\n",
    "        for offset in range(-window, window + 1):\n",
    "            neighbor_idx = idx + offset\n",
    "            if 0 <= neighbor_idx < len(all_chunks):\n",
    "                neighbors.add(neighbor_idx)\n",
    "    return sorted(neighbors)\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ”¥ THE SEARCH KING - MAIN FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_hybrid_search(\n",
    "    query: str, \n",
    "    top_k: int = 5,\n",
    "    use_hyde: bool = True,\n",
    "    use_query_expansion: bool = True,\n",
    "    use_reranking: bool = True,\n",
    "    use_neighbors: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ðŸ”¥ Advanced Hybrid Search with all the bells and whistles!\n",
    "    \n",
    "    Features:\n",
    "    - Query expansion (multiple query variations)\n",
    "    - HyDE (hypothetical document embeddings)\n",
    "    - Vector search (semantic)\n",
    "    - BM25 search (keyword)\n",
    "    - Reciprocal Rank Fusion\n",
    "    - Cross-encoder reranking\n",
    "    - Neighbor chunk retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ” ADVANCED SEARCH: '{query}'\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results = []\n",
    "    text_metadata = {}\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Query Expansion\n",
    "    # =========================================================================\n",
    "    queries = [query]\n",
    "    if use_query_expansion:\n",
    "        queries = expand_query_simple(query)\n",
    "        if verbose:\n",
    "            print(f\"ðŸ“ Query variations: {len(queries)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Multi-Query Vector Search\n",
    "    # =========================================================================\n",
    "    for q in queries:\n",
    "        # Standard embedding search\n",
    "        q_embedding = embed_text(q)\n",
    "        vector_results = search_vectors(q_embedding, limit=top_k * 2)\n",
    "        all_results.append(vector_results)\n",
    "        \n",
    "        # Store metadata\n",
    "        for r in vector_results:\n",
    "            text_metadata[r[\"text\"]] = {\n",
    "                \"source\": r.get(\"source\", \"\"),\n",
    "                \"metadata\": r.get(\"metadata\", {}),\n",
    "                \"chunk_index\": r.get(\"chunk_index\", 0)\n",
    "            }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸ§  Vector search: {sum(len(r) for r in all_results)} results\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: HyDE Search (Hypothetical Document Embeddings)\n",
    "    # =========================================================================\n",
    "    if use_hyde:\n",
    "        hyde_embedding = hyde_search(query)\n",
    "        hyde_results = search_vectors(hyde_embedding, limit=top_k * 2)\n",
    "        all_results.append(hyde_results)\n",
    "        \n",
    "        for r in hyde_results:\n",
    "            if r[\"text\"] not in text_metadata:\n",
    "                text_metadata[r[\"text\"]] = {\n",
    "                    \"source\": r.get(\"source\", \"\"),\n",
    "                    \"metadata\": r.get(\"metadata\", {}),\n",
    "                    \"chunk_index\": r.get(\"chunk_index\", 0)\n",
    "                }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ðŸŽ¯ HyDE search: {len(hyde_results)} results\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: BM25 Keyword Search\n",
    "    # =========================================================================\n",
    "    bm25_formatted = []\n",
    "    for q in queries:\n",
    "        bm25_results = search_bm25(q, top_k=top_k * 2)\n",
    "        for idx, score in bm25_results:\n",
    "            text = corpus_texts[idx]\n",
    "            bm25_formatted.append({\n",
    "                \"text\": text,\n",
    "                \"score\": score,\n",
    "                \"chunk_index\": idx\n",
    "            })\n",
    "            if text not in text_metadata:\n",
    "                text_metadata[text] = {\n",
    "                    \"source\": all_chunks[idx].metadata.get(\"source\", \"\") if idx < len(all_chunks) else \"\",\n",
    "                    \"metadata\": all_chunks[idx].metadata if idx < len(all_chunks) else {},\n",
    "                    \"chunk_index\": idx\n",
    "                }\n",
    "    \n",
    "    all_results.append(bm25_formatted)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸ”¤ BM25 search: {len(bm25_formatted)} results\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Reciprocal Rank Fusion\n",
    "    # =========================================================================\n",
    "    fused_scores = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    # Create combined results\n",
    "    combined_results = []\n",
    "    for text, rrf_score in fused_scores.items():\n",
    "        combined_results.append({\n",
    "            \"text\": text,\n",
    "            \"score\": rrf_score,\n",
    "            **text_metadata.get(text, {})\n",
    "        })\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    combined_results = sorted(combined_results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸ”— Fusion: {len(combined_results)} unique results\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Neighbor Chunk Retrieval\n",
    "    # =========================================================================\n",
    "    if use_neighbors and combined_results:\n",
    "        # Get top chunk indices\n",
    "        top_indices = [r.get(\"chunk_index\", 0) for r in combined_results[:top_k]]\n",
    "        neighbor_indices = get_neighbor_chunks(top_indices, window=1)\n",
    "        \n",
    "        # Add neighbor chunks if not already in results\n",
    "        existing_texts = {r[\"text\"] for r in combined_results}\n",
    "        for idx in neighbor_indices:\n",
    "            if idx < len(all_chunks):\n",
    "                chunk = all_chunks[idx]\n",
    "                if chunk.text not in existing_texts:\n",
    "                    combined_results.append({\n",
    "                        \"text\": chunk.text,\n",
    "                        \"score\": 0.1,  # Lower score for neighbors\n",
    "                        \"source\": chunk.metadata.get(\"source\", \"\"),\n",
    "                        \"metadata\": chunk.metadata,\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"is_neighbor\": True\n",
    "                    })\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ðŸ“ Added {len(neighbor_indices) - len(top_indices)} neighbor chunks\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: Reranking (if available)\n",
    "    # =========================================================================\n",
    "    if use_reranking and HAS_RERANKER:\n",
    "        combined_results = rerank_results(query, combined_results, top_k=top_k * 2)\n",
    "        if verbose:\n",
    "            print(f\"âš¡ Reranked with cross-encoder\")\n",
    "    \n",
    "    # Final top-k\n",
    "    final_results = combined_results[:top_k]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"âœ… Returning top {len(final_results)} results\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# =============================================================================\n",
    "# CONTEXT RETRIEVAL\n",
    "# =============================================================================\n",
    "\n",
    "def get_context_advanced(question: str, k: int = 3, verbose: bool = True) -> str:\n",
    "    \"\"\"Get formatted context using advanced search.\"\"\"\n",
    "    results = advanced_hybrid_search(\n",
    "        question, \n",
    "        top_k=k,\n",
    "        use_hyde=True,\n",
    "        use_query_expansion=True,\n",
    "        use_reranking=False,  # â† Change this to False\n",
    "        use_neighbors=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        source = os.path.basename(result.get(\"source\", \"Unknown\"))\n",
    "        text = result.get(\"text\", \"\")\n",
    "        score = result.get(\"score\", 0)\n",
    "        neighbor_tag = \" [NEIGHBOR]\" if result.get(\"is_neighbor\") else \"\"\n",
    "        context_parts.append(f\"[Source {i}: {source} | Score: {score:.3f}{neighbor_tag}]\\n{text}\")\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "# Keep original functions as aliases\n",
    "hybrid_search = advanced_hybrid_search\n",
    "get_context = get_context_advanced\n",
    "\n",
    "print(\"\\nðŸ”¥ SEARCH KING is ready!\")\n",
    "print(f\"   âœ… Query expansion: ON\")\n",
    "print(f\"   âœ… HyDE search: ON\")\n",
    "print(f\"   âœ… Hybrid (Vector + BM25): ON\")\n",
    "print(f\"   âœ… Reciprocal Rank Fusion: ON\")\n",
    "print(f\"   âœ… Neighbor chunks: ON\")\n",
    "print(f\"   {'âœ…' if HAS_RERANKER else 'âš ï¸'} Reranking: {'ON' if HAS_RERANKER else 'OFF (install sentence-transformers)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: ðŸŽ¯ Test Your RAG System!\n",
    "Enter a query to search your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ” ADVANCED SEARCH: 'What is meditation?'\n",
      "============================================================\n",
      "ðŸ“ Query variations: 4\n",
      "ðŸ§  Vector search: 24 results\n",
      "ðŸŽ¯ HyDE search: 6 results\n",
      "ðŸ”¤ BM25 search: 24 results\n",
      "ðŸ”— Fusion: 12 unique results\n",
      "ðŸ“ Added 6 neighbor chunks\n",
      "============================================================\n",
      "âœ… Returning top 3 results\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ðŸ“š RETRIEVED CONTEXT\n",
      "============================================================\n",
      "[Source 1: jd1.pdf | Score: 0.136]\n",
      "So then how is â€“ Iâ€™m using the â€˜howâ€™ merely as a question â€“ then how is one to find oneself in that?... Now, I think here comes the question of meditation. Iâ€™m not... we are not talking of meditation as a method â€“ you understand? â€“ therefore itâ€™s not... it has nothing whatever to do with method because method is the â€˜howâ€™ and we have pushed that aside as being inadequate, immature, juvenile.\n",
      "\n",
      "---\n",
      "\n",
      "[Source 2: jd1.pdf | Score: 0.120]\n",
      "r to arrive at that quietness is called generally meditation, which of course is too childish and...... itâ€™s too absurd. So... but yet I see the mind must be extraordinarily quiet because I know that any movement in any direction, at any level â€“ movement towards God, towards peace â€“ any movement is always within time.\n",
      "\n",
      "---\n",
      "\n",
      "[Source 3: jd1.pdf | Score: 0.117]\n",
      "ctice breathing indefinitely, it will still remain a stupid mind, and its gods will be still stupid.... So we are talking about a meditation which is a natural process... â€“ ah, sorry, not a process â€“ which is a natural thing. So if one has gone that far â€“ I hope some of us have; I donâ€™t know â€“ you will know for...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸŽ¯ ENTER YOUR QUERY HERE\n",
    "# ============================================\n",
    "query = \"What is meditation?\"  # <-- Change this to your question!\n",
    "\n",
    "# Get context\n",
    "context = get_context(query, k=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“š RETRIEVED CONTEXT\")\n",
    "print(\"=\" * 60)\n",
    "print(context)\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You have successfully built an end-to-end RAG pipeline!\n",
    "\n",
    "### What You Learned:\n",
    "1. **Loading** - Extract text from PDFs using pdfplumber\n",
    "2. **Chunking** - Split text into semantic pieces\n",
    "3. **Embedding** - Convert text to vectors using OpenAI\n",
    "4. **Vector Storage** - Store in Qdrant database\n",
    "5. **Hybrid Search** - Combine semantic + keyword search\n",
    "6. **Retrieval** - Get relevant context for any query\n",
    "\n",
    "### Next Steps:\n",
    "- Try different queries in Cell 10\n",
    "- Add more PDF files and rebuild the index\n",
    "- Adjust `CHUNK_MIN_SIZE` and `CHUNK_MAX_SIZE` in Cell 2\n",
    "- Experiment with `VECTOR_WEIGHT` and `BM25_WEIGHT`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
