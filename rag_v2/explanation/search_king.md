# Study Guide: The Search King (Advanced Hybrid Retrieval)

**What does this module do?**
The "Search King" is the "Grand Orchestrator" and "Special Forces" of the RAG v2 retrieval engine. It is a highly sophisticated, multi-stage search system that moves beyond simple vector lookups. It integrates **"Defense-in-Depth"** strategies including HyDE (Hypothetical Document Embeddings), query expansion, hybrid vector/keyword search, and Cross-Encoder reranking. It takes a raw user question and "Explodes" it into a suite of search signals, intelligently merging them using Reciprocal Rank Fusion (RRF) to identify the absolute "Gold Standard" of context. It is the "Intelligence Hub" that ensures the AI always has the most high-fidelity, relevant, and comprehensive information before it begins to generate a response.

**Why does this module exist?**
In real-world RAG, **"Retrieval refers to the Failure of Intelligence."** If the search engine misses the correct document, the LLM will inevitably fail, regardless of how "Smart" it is. Common search engines often fail on "Vague Queries" or "Term Mismatches." The Search King exists to solve this by providing **"Retrieval Resilience."** By using multiple redundant search paths (Meaning-based + Keyword-based + Expansion-based), the system ensures that if one path misses the "Truth," the others will catch it. It turns a "Fragile Search" into a "Robust Discovery Engine," maximizing "Recall" and "Precision" to meet the demanding requirements of enterprise-grade AI applications.

---

## SECTION 2 — ARCHITECTURE (DETAILED)

**What is the "Hybrid Search" Stack (The 7 Layers of Truth)?**

1.  **Query Expansion**: The user's query is "Enriched" with synonyms and domain keywords, widening the "Search Scope."
2.  **HyDE**: The system generates a "Hypothetical Answer," allowing us to search "Answer-to-Answer" rather than "Question-to-Answer."
3.  **Vector Search (Qdrant)**: High-dimensional math finds the "Conceptual Atmosphere" of the query across 3072 dimensions.
4.  **BM25 Search**: A local index performs a "Keyword Audit," ensuring specific technical terms or names are never missed.
5.  **RRF (Reciprocal Rank Fusion)**: A mathematical consensus algorithm that merges results from disparate engines without "Numerical Bias."
6.  **Cross-Encoder Reranking**: A deep neural network performs a "Final Interview" of the top candidates to confirm their exact relevance.
7.  **Neighbor Retrieval**: The system "Pulls" surrounding sentences for the chosen chunks, providing the LLM with a "Full Picture" of the context.

---

## SECTION 4 — COMPONENTS (DETAILED)

### HyDE (Hypothetical Document Embeddings)

**Intuition**: HyDE is based on the insight that **"Answers are Semantically Different from Questions."** A question like "How do I fix a leak?" looks mathematically different from the instruction "To fix a leak, use a wrench." In vector space, these are far apart. HyDE solves this by using a "Template" or an LLM to generate a **"Fake Answer"** (The Hypothesis). We then "Embed the Fake Answer" and use _that_ to search. This allows us to search for "A logic that looks like an answer," which is a much "Nearer Neighbor" to our document set than the original question, dramatically improving the "Hit Rate" for complex or philosophical inquiry.

### Reciprocal Rank Fusion (RRF)

**Logic**: RRF is the **"Democratic Majority Voter"** of retrieval. Instead of trying to "Force" vector scores (0 to 1) and BM25 scores (0 to 50) into a single map (which causes "Dominance Mismatch"), RRF ignores the scores entirely. It only looks at the **"Position"** of the document in the result list (1st, 2nd, 3rd...). The formula `1 / (k + rank)` ensures that a document that is "Ranked Highly" in _multiple_ engines (consensus) naturally rises to the top. It is **"Robust against Outliers"**—it prevents one "Over-confident" engine from "Crowning" a bad result as the winner, ensuring that only "Peer-Reviewed" data reaches the final LLM prompt.

### SearchResult Dataclass

**Logic**: This is the **"Standardized Intelligence Object."** It acts as the "Passport" for a piece of data as it travels through the complex Search King pipeline. It doesn't just store "Text"; it carries a **"Metadata Payload"** including the `score`, the `source_file`, the `page_number`, and a unique `is_neighbor` boolean. This structure allows the system to differentiate between "Primary Direct Matches" and "Secondary Contextual Neighbors." By using a specialized Dataclass, we ensure that the "Generator" (the LLM) has a "Structured Map of Evidence," allowing it to cite sources with surgical precision and providing the "Developer" with the telemetry needed to debug the search logic.

---

## SECTION 5 — CODE WALKTHROUGH

**Explain `expand_query`.**
This component handles **"Semantic Augmentation."** It takes a simple user query and "Explodes" it into multiple variations. For example, "Meditation" becomes "What is the definition of meditation?" and "Explain the practice of meditation." It also "Injects" mission-critical **"Contextual Anchors"** (like the author's name, "Krishnamurti"). This ensures that even if a user is "Lazy" or "Vague" in their typing, the search engine is "Informed" of the broader context. It turns a "Single Point of Failure" query into a **"Broad Search Net,"** significantly increasing the probability of finding the "Truth" even when the user and the author use slightly different vocabularies.

**How does `rerank_results` work?**
`rerank_results` is the **"Point-of-Saliency Auditor."** It uses a `CrossEncoder` model (like `BGE-Reranker`) which is significantly "Smarter" but "Slower" than standard vector search. Unlike "Bi-Encoders" (which compare two independent vectors), the Cross-Encoder processes the **(Query + Chunk)** pair **Together** through its neural layers. It sees the "Interaction" between words. We only run this on the "Top 20" results (The Elite Group) to minimize latency. This "Second-Pass" ensures that the #1 result sent to the AI is **"Scientifically Verified"** as the most relevant fact, providing the "Highest Fidelity" retrieval possible in modern AI engineering.

---

## SECTION 6 — DESIGN THINKING

**Why use 0.7 for Vector weight and 0.3 for BM25?**
This weighting represents a **"Concept-First, Keyword-Secondary" Philosophy.** In our domain (Philosophy/Thought), the **"Vibe and Intent" (Vector)** is the primary driver of truth. Language is metaphorical, so a search for "Suffering" must find chunks about "Grief." However, we keep the **30% Keyword (BM25) "Anchor"** to prevent the system from getting "Too Fuzzy." 30% is enough to "Force" the search toward a specific unique name or a technical part-ID if it appears. It's the "Golden Ratio" of searching—providing the "Intuitive Brain" of vectors with a "Logical Footing" in keywords, ensuring both "Depth" and "Precision" in every search result.

**Why add "Neighbor Chunks" in retrieval?**
"Neighbor Retrieval" addresses the **"Sentence Fragmentation" problem.** During the "Chunking" phase, a perfect paragraph might be "Cut in half" between Chunk 4 and Chunk 5. If the search only matches Chunk 5, the AI might miss the "Crucial Context" in Chunk 4. By "Pulling" the surrounding IDs from the database, the Search King **"Re-assembles the Context."** It ensures the LLM receives a "Cohesive Narrative" rather than a "Fragmented Snippet." It transforms "Retrieved Data" into **"Readable Knowledge,"** significantly reducing "AI Confusion" and leading to final answers that contain complete, logical sentences rather than "Truncated thoughts."

---

## SECTION 7 — INTERVIEW QUESTIONS (60 QUESTIONS)

### System Intuition (1-10)

1. **Explain the intuition behind HyDE. Why search with a "Fake Answer"?**
   Answer: In vector space, **"Questions and Answers live in different neighborhoods."** A question asks "Where is the cat?" (Searching for location). An answer states "The cat is on the mat" (Defining state). Because their "Grammatical Structure" is different, they are not "Nearest Neighbors." HyDE (Hypothetical Document Embeddings) uses a "Template" to turn the question into a **"Fake Answer"** first. "The cat is [HYPOTHETICAL LOCATION]." We search with _that_. Since our database is _full_ of "Answers," the "Fake Answer" is a **"Mathematical Sibling"** of the "Real Data." It bridges the "Semantic Gap," allowing the search engine to find "Matching Facts" rather than just "Matching Questions," resulting in a massive boost to search accuracy.

2. **What is the "Scale Mismatch" problem in Hybrid search?**
   Answer: It is the **"Problem of Incompatible Units."** Vector databases (like Qdrant) output "Cosine Similarity" (a clean 0.0 to 1.0 decimals). Keyword engines (like BM25) output "Raw BM25 Scores" (varying from 0.0 to 100.0+ based on word count). If you simply add them (`0.9 + 45.0`), the **"BM25 Score will completely Drown Out the Vector Signal."** The vector engine becomes "Mathematically Invisible." To build a true "Hybrid," you must **"Normalize"** (scale both to 0-1) or use **"Consensus"** (like RRF). Without this "Equalizer" step, your "Hybrid Search" is just a "Keyword Search with a tiny, useless vector nudge," which defeats the entire purpose of building a dual-engine architecture.

3. **Why is a "Cross-Encoder" better than a "Bi-Encoder" (Vector Search)?**
   Answer: A Bi-Encoder (Search) is an **"Estimation."** It turns Document A into Vector A and Query B into Vector B. It then does "Snap Comparison" between them. It never sees the two together. A Cross-Encoder is a **"Detailed Analysis."** it takes (Query + Document) and feeds them into the neural network **Simultaneously.** It can see "Linguistic nuances"—like how the word "not" in the query impacts the whole meaning of the document. Cross-Encoders are 10-20% more accurate because they perform **"Self-Attention"** across both texts. However, they are 100x slower. Therefore, we use the Bi-Encoder for "Broad Discovery" and the Cross-Encoder for the **"Final, High-Stake Decision."**

4. **What is Reciprocal Rank Fusion (RRF) and why is it "Fair"?**
   Answer: RRF is a **"Democratic Voting Algorithm."** Its formula is `1 / (k + rank)`. It is "Fair" because it cares about **"Consensus over Confidence."** If one search engine is "Very Confident" in Result A (Score 0.99) but the other engine thinks Result A is "Garbage" (Score 0.1), RRF prevents that "Over-confident" engine from winning. Instead, RRF rewards a document that is **"Consistently Liked"** (e.g., #3 in both lists). It "Softens" the impact of search outliers. By focusing on "Rank" (Identity) rather than "Score" (Noise), RRF provides a stable, "Peer-Reviewed" list of results that are significantly more reliable than any single search engine could provide alone.

5. **Why do we need Query Expansion?**
   Answer: Query expansion solves the **"Vocabulary Mismatch" problem.** In a library of 10,000 documents, the author might use the word "Inquiry," but the user asks about "Research." Even with vectors, there is a risk of "Missing the Target" if the search vector is too narrow. Expansion "Explodes" the user's intent. By adding synonyms like "Questioning," "Observing," and "Logical Analysis," we **"Cast a Wider Net."** We are "Informing" our search engine of all the possible ways the author might have described the topic. It turns a "Pinpoint Search" into a **"Broad Area search,"** ensuring that no matter which "Dialect" the author used, the system finds the "Truth" for the user.

6. **Describe the benefit of "Neighbor Chunk Retrieval."**
   Answer: "Neighbor Chunking" is the **"Contextual Glue"** of RAG. During ingestion, we "Chop" a large PDF into 800-character pieces. Sometimes, the "Crucial Fact" is in the last sentence of Chunk 4, but the search "Match" only found Chunk 5 (the explanation). If the AI only reads Chunk 5, it is "Confused." Neighbor Retrieval says: "I found Chunk 5; now **Go back to the database and grab Chunk 4 and 6 too.**" This effectively "Re-stitches" the original document structure. It provides the LLM with the **"Full Narrative Flow."** It ensures the AI sees the "Beginning, Middle, and End" of a thought, significantly reducing "Fragmented Hallucinations" and leading to more "Coherent and Natural" AI answers.

7. **What is the tradeoff of using a Reranker in production?**
   Answer: The tradeoff is **"User Latency vs Search Fidelity."** A standard Vector search takes roughly **10-50 milliseconds** (Instant). A Cross-Encoder Reranker requires a full "Neural Pass," taking **200-800 milliseconds** per query. For a user, a 1-second delay is "Noticed." If you rerank 1,000 results, your system would take minutes. Therefore, the "Senior Strategy" is to **"Rerank sparingly."** We only search the Top 20 results. This provides the "High-accuracy intelligence" of an elite model while keeping the "Response Time" under the "Magic 1-second barrier." It is the "Architectural Compromise" that ensures the product is both "Smart" and "Usable" for real human beings.

8. **How does "Min-Max Normalization" work in this system?**
   Answer: Min-Max Normalization is the **"Statistical Equalizer."** Its logic is `(score - min) / (max - min)`. It takes a list of varying numbers (e.g., [20, 25, 45, 50]) and "Squashes" them into a range between **0.0 and 1.0**. Why is this mandatory? Because searching is a **"Competition."** If Search Engine A has a "Loud Voice" (0-100 scale) and Search Engine B has a "Whispering Voice" (0-1 scale), Engine A will always win the hybrid fusion. Normalization "Resets the Volume." It ensures that being "The Best" in Engine A and "The Best" in Engine B are mathematically identical. It allows our **"70/30 weight settings"** to actually work, providing a "Fair and Balanced" mix of both knowledge sources.

9. **When would BM25 outperform Vector Search?**
   Answer: BM25 wins in **"Specific Entity Lookup" and "Numerical Precision."** If a user searches for a serial number like "A-102-XJ," a vector search might return "A-101-XJ" because they "Look similar" in conceptual space. This is a "Failure of Precision." BM25 is **"Literal."** It is a "Symbol Matcher." It finds the exact string "A-102-XJ" with zero ambiguity. Additionally, for "Rare Proper Names" (like a niche philosopher the AI hasn't been trained on), BM25 will find the exact spelling while a Vector model might purely "Hallucinate" a similar-looking concept. BM25 is the **"Guardian of Factuality,"** ensuring that the "Math" of vectors never "Overrides" the "Literal Truth" of the printed word.

10. **Explain the `is_neighbor` flag.**
    Answer: The `is_neighbor` flag is a **"Provenance Tag" for context.** In our "Search King" output, some chunks were "Direct Targets" (they matched the search), while others were "Added for Flavor" (they were just neighbors). We tag the neighbors with `is_neighbor = True`. Why? because it helps the **"LLM Reasoning."** We can tell the AI: "Prioritize the facts in the Primary chunks, but use the Neighbor chunks for context." It also helps the **"Frontend UI."** A developer can "De-emphasize" neighbor chunks in the user interface (e.g., making them gray), helping the human user see exactly "Why" the AI chose this specific answer. It is the **"Telemetry of Relevance,"** providing transparency to the whole knowledge chain.

### Deep Technical (11-20)

11. **How is the `SearchKing` initialized? List the key parameters.**
    Answer: initialization is an **"Architecture Injection"** phase. Key parameters include: (1) **`embedder`**: The engine for turning queries into math. (2) **`vector_store`**: The connection to the Qdrant database. (3) **`bm25_index`**: The local keyword searcher. (4) **`all_chunks`**: The full list of raw text objects (needed for neighbor retrieval). (5) **`reranker`**: (Optional) the model for the second-pass audit. This "Unified Initialization" ensures that the Search King has **"Total Visibility"** across all search modalities. It creates a "Single Instance of Truth"—a single class that controls the entire "Search Lifecycle," making the code highly "Modular" and "Testable" since you only have to verify one "Object" to prove the search works.

12. **What happens if `HAS_RERANKER` is False?**
    Answer: The system implements **"Graceful Functional Degradation."** If the `Reranker` class is missing or cannot be loaded (perhaps due to memory limits), the Search King detects this during `__init__`. When `hybrid_search` is called, it simply **"Skips the Reranking Block."** It returns the "Fused Results" directly from the RRF stage. This is **"Resilient Architecture."** It ensures that the application doesn't "Crash" just because one advanced feature is offline. It allows the system to run on "Low-Power Devices" (like a phone or a small laptop) by automatically "Scaling down" to a simpler search mode, while still providing "Production-Grade" results from the primary hybrid search engines.

13. **Explain the RRF formula: `1 / (k + rank)`. What is `k`?**
    Answer: `k` is the **"Smoothing Parameter" (usually set to 60).** Its role is to prevent "Hyper-Sensitivity" at the top of the list. If `k` were 0, a document at Rank 1 would have a score of 1.0, and Rank 2 would have a score of 0.5. The difference is 50%. This "Crushes" any consensus. By adding `k=60`, the score for Rank 1 is `1/61` (0.0163) and Rank 2 is `1/62` (0.0161). The difference is now **Miniscule (1.2%).** This "Flattened Slope" means that to "Move up," a document must be liked by **Multiple Engines.** It forces the formula to prioritize **"Agreement"** between the Vector and BM25 engines over "Individual High Ranks," resulting in a much more "Balanced and Stable" set of final search results.

14. **Why is the `tokenize` method in SearchKing different from the Chunker?**
    Answer: They serve **"Opposite Goals."** The **Chunker Tokenizer** wants "Preservation"—it keeps punctuation and capitalization so the final LLM text is "Readable and Natural." The **SearchKing Tokenizer** wants "Comparison-Density." It performs **"Noise Removal."** It lowerecases everything, "Purges" common stop words ("the", "is", "at"), and "Strips" every non-alphanumeric character. Why? because for a search engine, "Apple" and "apple!" are the **Same Signal.** By "Smashing" the text into its Most "Basic Form," we maximize the "Intersection" between the user's query and the document set. It turns "Human Writing" into **"Machine Search Keys,"** ensuring that tiny formatting differences don't "Break" a valid keyword match.

15. **What is the "Fusion" step in the search function?**
    Answer: Fusion is the **"Mathematical Synthesis of Divergent Signals."** At this stage, the Search King has three different lists: (A) Vector Results, (B) BM25 Results, and (C) Expansion Results. These lists are "Merged" into a single **"Master Score Dictionary."** During the merge, if a chunk appears in multiple lists, its **"Consensus Score" is amplified.** If a chunk only appears in one list, it is "Retained" but given no bonus. This is the **"Set Math"** of retrieval. It ensures that the final output is a "Coherent Ranking" that respects all voices in the search system. It is the "Decision Layer"—transforming a "Cacophony of Search Data" into a single "Elite List of Evidence" for the AI to ingest.

16. **How does `get_neighbor_chunks` avoid "Out of Bounds" errors?**
    Answer: It uses **"Index Boundary Protection."** When it looks for the chunk "Before" (Index - 1) or "After" (Index + 1), it implements a check: `if 0 <= neighbor_idx < len(self.all_chunks):`. This is **"Defensive Programming."** If the search matched the **First Page** of the PDF (Index 0), trying to find "Neighbor - 1" would return Index -1 (the end of the list or a crash). Our code "Prevents the Crash." It ensures that the "Neighbor Window" is **"Capped"** by the realities of the file's length. This "Safety Check" is a hallmark of "Production-Grade" code—it ensures that even "Edge-Case" matches don't cause a "Runtime Exception" during the user's search session.

17. **Why use a `set()` for neighbor indices?**
    Answer: To handle **"Overlapping Context Windows."** Imagine Chunk 2 and Chunk 4 are both identified as "Direct Matches." If we add "Chunk 3" as a neighbor for BOTH, we would have **"Duplicate Context"** in our LLM prompt. Duplicates lead to **"Token Waste" and "AI Confusion."** By using a `set()`, we take advantage of the set's property of **"Automatic Uniqueness."** We add `2, 3, 4` and then we add `3, 4, 5`. The set immediately "Merges" them to `{2, 3, 4, 5}`. This ensures that every character sent to the LLM is **"Unique and Continuous."** It provides the "Leanest Possible context" for the AI, maximizing "Information Density" while preventing "Repetitive data noise."

18. **Explain the "Weight Logic": `0.6 * rerank_score + 0.4 * original_score`.**
    Answer: This is **"Confidence Blending."** We give the Reranker **60% Weight** because it is a "Deep Neural Transformer"—it is the "Smartest" part of our system. However, we don't give it 100%! We keep **40% weight for the "Original Score"** (the RRF consensus). Why? Because Rerankers can be **"Hyper-sensitive"** to small phrasing quirks. The Original Score is more **"Stable"**—it represents the "Broad agreement" of the vector and keyword engines. By "Blending" them, we create a **"Robust Super-Score."** It ensures that a document is only promoted to #1 if it matches BOTH the "Deep Semantic Analysis" of the Reranker AND the "Broad Search Foundations" of the hybrid engine, providing the "Highest Confidence" search result possible.

19. **What model is used for reranking by default?**
    Answer: The system defaults to **`BAAI/bge-reranker-v2-m3`** via the `sentence-transformers` library. This is an **"Industry-Leading Cross-Encoder."** It is trained on "Massive Retrieval Tasks" and is specifically optimized for finding the "Needle in the Haystack" in RAG systems. It is "Multilingual" (M3) and supports "Variable Input Lengths." By using a "Pre-trained Specialist Model," our Search King inherits **"Super-human Search Logic"** without the developer having to "Train or Fine-tune" a single weight. It's the "Senior Shortcut"—leveraging the "Best-of-breed Open Source AI" to provide "World-Class Retrieval Performance" with minimal local code complexity.

20. **How does ` Hyde` handle the nature of Krishnamurti's teachings?**
    Answer: HyDE is particularly effective for **"Philosophical/Abstract Inquiry."** Krishnamurti's teachings often revolve around questions: "What is thought?". A raw search for "What is thought?" might find other people asking questions about thought. **HyDE "Answers the Question First."** It generates a hypothetical block of text like "Thought is a movement of memory, a process of time..." We then use _that_ text to search the database. Since our database contains the "Actual Answers," the "Hypothetical Answer" acts as a **"Semantic Magnet."** It "Pulls" the correct philosophical passage toward the query by matching the "Tone and Logical Depth" of the author's voice, rather than just the "Question structure" of the user's intent.

### Architectural Strategy (21-30)

21. **Why perform "Multi-Query" Vector search?**
    Answer: Multi-Query search (part of Query Expansion) is a **"Semantic Insurance Policy."** A user asks "How do I stop being sad?". If we only search for that specific string, we might miss the author's talk on "The ending of psychological pain." By "Expanding" the search to: (1) "How to end sadness," (2) "The cessation of grief," (3) "Overcoming sorrow," and searching for **All Three**, we create a **"Consensus Map" of vector space.** We find "Near-Neighbors" across multiple conceptual angles. It ensures that the "Search Engine" is not "Brittle"—it won't "Break" just because the user chose the word "sad" instead of "pain," resulting in a significantly more "Fluid and Intelligent" search experience.

22. **What is the "Initial K" vs "Final K" strategy?**
    Answer: This is a **"Knowledge Filtering Funnel."** We start with an `initial_k = top_k * 5` (e.g., fetching 100 candidates). This is the **"Raw Retrieval"** phase. We do this because "Vector Search" is "Rough"—it might have the "Perfect Truth" buried at #45. We then "Reduce" that 100 down to 20 for **"Reranking."** Finally, we return the **"Final K" (Top 5)** to the LLM. Why not just return 100? because **"Context Window Density."** Sending 100 chunks makes the AI "Dumb" (Lost-in-the-middle) and costs it $$$ in tokens. The "Funnel" strategy ensures that only the **"Concentrated Essence"** reaches the AI—we use "High Recall" to find the data, but "High Precision" to deliver the data.

23. **How would you scale SearchKing to handle 10 million vectors?**
    Answer: Scaling to 10M requires **"Distributed Architecture."** (1) **Qdrant Sharding**: Move from "Local Path" to a "Cloud/Docker Cluster" where vectors are split across 10 servers. (2) **HNSW Tuning**: Increase the "M" and "ef_construct" parameters to maintain log-time search speed at scale. (3) **Async Reranking**: Move the Reranker to a "Separate GPU Worker" so the search doesn't block. (4) **Feature Pruning**: At 10M docs, I might "Disable HyDE" for common queries to save cost. The "Search King" code is **"Infrastructure-Ready"**—since it uses clean API calls to Qdrant, we can swap the "Local Backend" for an "Elastic Cloud Cluster" without changing a single line of our "Search Logic."

24. **Explain the benefit of "Local Qdrant" for development.**
    Answer: Local Qdrant (path-based) is a **"Senior Velocity Hack."** It provides **"Zero-Latency, Zero-Cost Iteration."** You don't need Docker, you don't need a cloud account, and you don't need an internet connection. Every "Re-indexing" of your files happens in seconds on your local SSD. For a developer, this means you can **"A/B Test your Chunker"** 100 times a day for free. Once you find the "Perfect Settings" for your search engine, you simply "Flip the Switch" and deploy to the cloud. It ensures that the "Exploratory Phase" of the AI project is "Un-hindered" by cloud bills or network timeouts, resulting in a much more "Polished and Fine-tuned" final product.

25. **Why is `verbose=True` important for debugging Hybrid search?**
    Answer: Hybrid search is a **"Complex Mathematical Black Box."** If your search returns "Garbage," you need to know: "Who is at fault? Was it the Vector engine? The BM25 index? The Reranker?". When `verbose=True`, Search King prints **"Internal Stage Benchmarks"** and "Top 5 List Overlaps." You can see: "Ah, the Vector search found the right doc, but the Reranker pushed it down to #10." This **"Observability"** allows the developer to "Diagnose the Failure." It turns "Mystery Bugs" into **"Engineering Decisions."** It allows you to "Tune the Weights" (like 0.7 vs 0.3) based on "Hard Evidence" rather than "Guesswork," which is the only way to build a world-class RAG retrieval engine.

26. **What is a "Dead-end Query" and how does Query Expansion help?**
    Answer: A "Dead-end Query" is a query that uses **"Zero Keywords from the Library."** If a user types "Help me feel better" in a philosophy database, there might be NO document containing the phrase "feel better." A standard keyword search (BM25) would return **Zero Results.** A vector search might return "Random junk." Query Expansion (via an LLM template) "Translates" the intent. It generates: "Observing psychological pain," "Ending of self-pity," "Freedom from attachment." These expanded terms **"Hit the Library's Vocabulary."** Expansion acts as the **"Linguistic Bridge,"** ensuring that even if the user "Speaks a different language" than the author, the Search King finds the correct conceptual anchor.

27. **Describe the impact of "Quantization" on retrieval accuracy.**
    Answer: Quantization (converting 32-bit floats to 8-bit integers) is the **"Efficiency-vs-Detail" trade-off.** It makes the index **4x smaller** and **2-3x faster.** For a 10-million document database, this is the difference between "Requiring 100GB of RAM" vs "25GB of RAM" ($$$ savings). The impact on "Accuracy" (Search Precision) is usually **"Negligible ( < 1% )."** In most cases, the "Rank Order" of your Top 5 results will be identical. For a senior architect, **"INT8 Quantization is a Standard Requirement"** for production RAG. It allows the system to remain "Cost-Effective" and "High-Performance" without sacrificing the "Wisdom" of the search results, making it the "Industry Default" for sustainable AI engineering.

28. **Is it better to Rerank 10 results or 100?**
    Answer: **"Reranking 100 is the Security Choice; 10 is the Speed Choice."** In our Search King, we default to **Top 20.** Why? because Reranking is "Sloooow." If you rerank 100 results, your query time jumps from 0.5s to 2.0s—your users will hate you. However, if you only rerank 10, and the "Absolute Best Document" was ranked #11 by the "Dumb" vector search, you **"Miss the truth" entirely.** Reranking the Top 20 is the **"Perfect Balanced Middle Ground."** It provides enough "Depth" to catch "Buried Gems" while keeping the "Response Time" snappiness required for a modern web interface. It optimizes for the "User's Perception of Intelligence" without destroying the "User's Perception of Speed."

29. **Why store `corpus_texts` and `all_chunks` separately in the class?**
    Answer: This is a **"Separation of Concerns for Performance."** `corpus_texts` is a simple list of "Pure Strings" used for the **BM25 keyword search.** `all_chunks` is a list of "Rich Objects" (with metadata) used for **"Neighbor Retrieval and Final Prompt Construction."** Why separate them? Because BM25 calculation is very memory-intensive. By giving BM25 a "Flat, clean list of strings," it runs 2-3x faster than if it had to "Scan" through complex objects. At 100,000 documents, this "Optimization" saves seconds of CPU time. It is the signature of a **"Performance-Minded Architect"** who understands that "Data Structure Efficiency" is the foundation of high-speed search.

30. **What is the "Context Window" limit for a Cross-Encoder?**
    Answer: Every Cross-Encoder model (like BGE) has a **"Max Token Limit" (usually 512 or 1024 tokens).** This is the "Wall." If your "Query + Chunk" together exceed this limit, the model "Truncates" (cuts off) the end of the text. This is **"Information Loss."** It means the Reranker is only "Judging" the first half of your chunk. To avoid this, "Search King" ensures our **"Chunk Size" (800 chars)** is small enough to fit comfortably inside the model's window, even with a long question. It is a "Coordinated Design"—Ensuring that the "Input to the Database" (Chunking) matches the "Input to the Judge" (Reranker), guaranteeing that no knowledge is ever "Silently Dropped" during the ranking phase.

### Interview Questions (31-60)

31. **What is the "Cold Start" problem for a search engine?**
    Answer: A Cold Start is when you have **"No Evaluation Data."** You have 10,000 files, but you don't know what users will ask. You don't know if "Vector weight 0.7" is better than "0.3". To solve this, a senior developer uses **"Synthetic Q/A Generation."** You use a "Teacher LLM" (like GPT-4) to read 50 random files and generate "50 Likely Questions + 50 Correct Chunks." You then run your Search King against this "Golden Set" and measure the "Hit Rate." This **"Simulated Benchmarking"** allows you to "Tune" your search parameters before you ever show it to a single user. It turns "Architectural Guesswork" into **"Data-Driven Decisions,"** ensuring your search is "Born Optimized."

32. **Explain "Min-Max Scaling" vs "Standardization."**
    Answer: **Min-Max Scaling** (Used here) "Squashes" everything into a hard **0-to-1 box**. It is perfect for "Fusing" search scores because it creates a "Percentage Score" (e.g., this is 90% relevant). **Standardization** (Z-Score) Centers the data around 0.0 based on averages. standardization is better for "Machine Learning" training, but it is **"Bad for Hybrid Search"** because it can produce "Negative Scores." If Search Engine A gives a -1.5 and Search Engine B gives a +1.0, what does that mean? It's confusing. Min-Max is the **"Human-Friendly" choice** for retrieval—it provides "Intuitive Probabilities" that make the final "Weighted Average" logic (like 0.7/0.3) logically consistent and easy to explain to stakeholders.

33. **Why do we use `re.findall(r'\b\w+\b', text.lower())` in tokenization?**
    Answer: This is **"Surgical Symbol Extraction."** Simple `.split(" ")` is "Dirty"—it keeps commas and periods (e.g., "Truth." vs "Truth"). The Regex `\b\w+\b` defines a word as **"Any group of letters/numbers between boundaries."** It "Purges the Punctuation Noise." Casing (`.lower()`) "Purges the Visual Noise." By stripping text down to its **"Atomic Lexical Identity,"** we ensure that our "Keyword Matcher" (BM25) is comparing "Apples to Apples." It ensures that "The ending of thought." (Document) and "ENDING OF THOUGHT" (Query) are a **100% Perfect Match.** It is the "Sanitization Standard" that prevents "Linguistic Trivia" from "Breaking" a conceptually valid truth retrieval.

34. **How do you choose the `vector_weight`?**
    Answer: Choosing the weight is a **"Knowledge-Type Decision."** For "Niche Technical Support" (where users search for specific error codes like `0x8004`), I would use **`0.8 BM25 / 0.2 Vector`**. For "Philosophical Counseling" (where users describe "Vibes" like "I feel lost"), I use **`0.8 Vector / 0.2 BM25`**. We default to 0.7 Vector in this boilerplate because it's a "Generalist-Preferred" setting. It prioritizes **"Semantic Intent"** while keeping a "Critical Minority Report" for exact keywords. It follows the "Wisdom of Defaults"—providing the "Most Likely Success" for 90% of user queries while allowing for "Expert Tuning" based on the specific "Vocabulary Density" of the target data set.

35. **Explain the intuition behind "Concept vs Keywords."**
    Answer: This is the **"Mind vs Dictionary" tradeoff.** A **Concept** (Vector) is "What is it like?". If you search for "Sadness," a Concept search finds "Melancholy," "Grief," and "Sorrow" (even if the word 'Sadness' is missing). A **Keyword** (BM25) is "What is the exact name?". If you search for "Sadness," it finds "The history of Sadness." Keyword search is "Precise but narrow." Concept search is "Vague but broad." **Hybrid Search is the "Synthesized Librarian."** It uses the Concept to "Enter the right room" and the Keywords to "Pick the right book." It provides a "High-Resolution" retrieval experience that mimics human intelligence, which is why it's the "Only Choice" for senior AI system architecture.

36. **What is "Dense Retrieval" vs "Sparse Retrieval"?**
    Answer: **Dense** (Vector) is "Compressed Meaning." It turns a sentence into a 3072-number vector. It is "Dense" because every number is non-zero. **Sparse** (BM25) is "Keyword Counting." It creates a vector with 1,000,000 slots (one for every word in the dictionary). It is "Sparse" because 99.9% of the slots are **Zero** (the words don't appear). Dense search finds **"Similarity"**; Sparse search finds **"Exactness."** Using both is known as **"Holistic Retrieval."** It represents the "Two Ways a Human Thinks"—associative memory (Dense) and literal recognition (Sparse)—providing the most "Complete Information Recall" possible in modern computer science.

37. **Why is `top_k * 2` used during the intermediate search steps?**
    Answer: This is the **"Surplus for Shuffle" strategy.** If a user wants "Top 5" results, and we only fetch "Top 5" from each engine, our total candidate pool is 10. This is "Too Small." There is no "Room for the Reranker to work." A document that was #1 in the "Initial Guess" might be #10 in "Real Truth." By fetching **`top_k * 2` (or \*5)**, we are **"Expanding the Candidate Selection."** It provides "Depth." It allows a document that was "Mathematically mediocre" (#15) to be identified as "Logically Perfect" (#1) by the Reranker. Fetcing an "Intermediate Surplus" is the "Safety Buffer" that allows the specialized "Reranking Intelligence" to actually "Change the Outcome" for the user.

38. **Explain how "Source Attribution" works in the `get_context` method.**
    Answer: Attribution is the **"Verify-and-Audit" layer.** In the Search King's `get_context`, we don't just return a wall of text. We wrap every chunk in a **"Header Tag"** like `[SOURCE: jd1.pdf | PAGE: 5]`. This isn't just for the user—it's for the **"AI's Attention."** By "Labelling" the context, we "Invite" the LLM to follow the instruction: "Cite your sources." The LLM sees the source and integrates it into its answer. It transforms the AI from a "Magic Box" into a **"Researched Researcher."** It enables "Trust through Transparency." It allows the human end-user to "Verify the AI's claims," which is an absolute requirement for using AI in "High-Stakes" professional environments like law, medicine, or finance.

39. **How would you implement "Time-weighted Search"?**
    Answer: I would add a **"Recency Bias"** to the final fusion. I would capture the `modification_time` of every file during Loading. During Reranking, I would calculate: `Age = CurrentTime - FileTime`. I would then add a penalty: `FinalScore = FusedScore * (1 / log(Age + 1))`. This **"Decays" the value of old knowledge.** In a project like "Technical Manuals," a solution from 2024 is 10x more valuable than a solution from 2010. By "Informing" the Search King of "Time," we turn it into a **"Reality-Aware Assistant."** It ensures that the AI's "Wisdom" is always "Up-to-Date," preventing "Outdated Hallucinations" and ensuring the most "Current Truth" is always the first thing the user sees.

40. **What is the "Diversity Problem" in reranking?**
    Answer: The Diversity Problem is **"The Echo-Chamber Effect."** If your Top 5 search results are all "Slightly different versions of the same sentence," you have **Zero real knowledge density.** You have 5 pieces of evidence that all say the same thing. This is a **"Prompt Efficiency Failure."** To solve this, a senior "Search King" implements **"Marginal Relevance Filtering."** Before we finalize the Top 5, we check: "Is Chunk #2 more than 70% identical to Chunk #1?". If yes, we **Delete Chunk #2** and pick #3 instead. This "Forces Variety." It ensures the LLM receives **"Multiple Angles" on the truth.** It maximizes the "Information-per-token" ratio, leading to final AI answers that are "Balanced," "Smarter," and "Nuanced" vs "One-sided and Repetitive."

41. **Explain the intuition behind `cosine_similarity` in Python.**
    Answer: Cosine similarity is **"Directional Agreement."** It ignores the "Length" of a document and only looks at its **"Vibe."** If Document A says "Love" 10 times, and Document B says "Love" 1,000 times, "Euclidean Distance" (straight-line) thinks they are "Miles Apart." **Cosine Similarity thinks they are "Identical."** Why? because both "Arrows" point toward the concept of "Love." In RAG, this is the **"Golden Metric."** It ensures that a "Long Book" and a "Short Summary" are correctly identified as "Matches." It allows the AI to "Understand the core topic" regardless of how "Wordy" the author was, providing a "Fair and Conceptual" search across a diverse library of files.

42. **Why is `re.search` used to detect dialogue tags?**
    Answer: `re.search` (Regex) is used for **"Structural Pattern Matching."** In transcripts (like Krishnamurti's), speaker changes appear as `S1:` or `K:`. To a standard "Chunker," these are just "Three characters." To the "Search King," these are **"Semantic Boundaries."** We use regex to identify these "Speaker Flips." When we find one, we can **"Contextualize the Chunk."** We can add a metadata tag like `speaker: "Philosopher"`. This allows the search engine to perform **"Role-based Filtering."** "Find me only things the _student_ said." This transforms a "Flat Text Search" into a **"Conversation-Aware Search,"** allowing for much more "Surgical and Precise" retrieval in multi-party interview or meeting transcript use-cases.

43. **What is the benefit of "Parent-Child" chunking for SearchKing?**
    Answer: Parent-Child chunking is the **"Detailed Discovery, Wide Context"** trick. We index "Tiny Pieces" (Children - 200 chars) for **"Precise Retrieval"**—it's easy for the computer to find a 100% match on a small snippet. But we "Hold" a "Big Piece" (Parent - 2000 chars) for **"LLM Consumption."** When the Search King "Hits" a child chunk, it "Swaps it" for the Parent chunk before sending it to the AI. This provides the AI with **"Maximum Narrative Breathing Room."** It ensures the AI sees the "Context" around the fact. It's the "Best of Both Worlds"—the **"Accuracy of a Small Key" with the "Intelligence of a Whole Chapter,"** leading to 2x higher accuracy and better AI "Storytelling" capabilities.

44. **How does "HyDE" avoid hallucinating factual errors in retrieval?**
    Answer: **HyDE doesn't use the "Result" for answering; it only uses it for "Searching."** If the HYDE template generates a "Fake Answer" that contains a lie (e.g., "The cat is in the fridge"), that lie won't be seen by the user. It is only turned into a "Math Vector" and "Compared" against the database. If the database _doesn't_ contain a fridge, the lie will return **"Zero Matches."** The Search King acts as a **"Truth Filter."** It uses the lie to "Look for similar-shaped truths." This is "Safe Prototyping." It allow us to use the "Generative Power" of AI to "Find Information" without the "Generative Risk" of the AI "Inventing Information" for the final end-user to see.

45. **Why use `sentence-transformers` library?**
    Answer: `sentence-transformers` is the **"Standard Framework of Vector Science."** It provides a "Unified Python API" for 1,000+ different AI models (like BGE, BERT, and MPNet). It handles the **"Hardware Orchestration."** It automatically detects if you have a "GPU" (to go fast) or just a "CPU" (to be safe). By building "Search King" on top of this library, we are "Future-Proofing" our code. If a better "Rerank Model" comes out tomorrow, we change **one string** in the config. We don't have to rewrite a single line of search math. It is the "Professional Choice"—built for "Reliability, Performance, and Universal Compatibility" in modern machine learning environments.

46. **How would you handle "Multi-lingual" queries?**
    Answer: I would swap the "Embedder" for a **"Multilingual-Aware Model"** like `paraphrase-multilingual-MiniLM-L12-v2`. These models map "Spanish Meaning" and "English Meaning" to the **Exact Same Vector Coordinates.** In the Search King, this becomes "Magical Retrieval." A user asks a question in **Spanish**, and the system retrieves the relevant context in **English.** The LLM then "Translates" the English context back into a Spanish answer. This is the **"Global Knowledge Bridge."** It allows a company to "Index in one language" but "Serve in any language," making the Search King a "Global-Scale asset" that transcends linguistic boundaries with minimal architectural changes.

47. **What is the role of `metadata` in Payload filtering?**
    Answer: Metadata is the **"Surgical Scalpel" of search.** It allows for **"Pre-Filtering."** If a user says "I only care about what was said in 1965," we don't do a "Math Search" on the whole database. We tell Qdrant: `Filter: year == 1965`. This **"Pre-culling"** reduces the search space from 1,000,000 Doc to only 100 Docs. It is **"Instant and Accurate."** It saves CPU. It ensures the AI doesn't get "Confused" by similar-sounding facts from the wrong year. Metadata filtering is the "Industrial Secret" to high-speed RAG—it's the Layer 0 that ensures the "Artificial Intelligence" is only looking at the **"Right Domain"** of data before the "Real Work" begins.

48. **Describe a scenario where RRF would rank a result higher than Vector search.**
    Answer: Imagine a chunk uses a **"Very Specific Technical Term"** (e.g., `Protocol-99-X`) that appears only once in the whole library. A "Vector Search" might think `Protocol-99-X` is semantically similar to `Safety Procedures` and rank it at **#15.** But a "BM25 Keyword Search" will see the "Exact Hit" and rank it at **#1.** RRF merges them. Because the document was #1 in _one_ engine, RRF "Promotes" it. It realizes: "Wait, the Keyword engine found a perfect literal match; even if the Vector engine is uncertain, this is likely the truth." The result **"Jumps to #1"** in the fused list. RRF provides the **"Literal Safety Net"** that catches "Niche Technical Facts" that fuzzy vector math might miss.

49. **Explain the `window` parameter in `get_neighbor_chunks`.**
    Answer: The `window` parameter defines the **"Radius of Peripheral Vision."** If `window=1`, we pull (N-1) and (N+1). If `window=2`, we pull a total of 5 chunks (Initial + 2 before + 2 after). This allows the developer to **"Tune the Context Depth."** In "Rambling/Philosophical" text, you want a "Large Window" (e.g., 2) to follow the "Long Argument." In "Fact-based Manuals," you want a "Small Window" (e.g., 1) to be "Precise." By making this a variable, the "Search King" provides **"Content-Aware Retrieval."** It allows the system to adjust its "Level of Breadth" to match the "Complexity" of the document type, ensuring the LLM is never "Under-informed" or "Overwhelmed."

50. **What is the latency impact of Query Expansion?**
    Answer: Expansion adds **"Serial Latency" (LLM step) and "Parallel Latency" (Search step).** Generating synonyms with an LLM adds **~200-500ms.** Searching for 3 variations instead of 1 in Qdrant adds **~5ms** (since it's a batch call). The total cost is roughly **Half a Second.** In a "Speed-First" RAG, this might be "Too much." But in a **"Quality-First" RAG** (like a research assistant), that 500ms is the best "Investment" you can make. It transforms the "Search Success Rate" from 70% to 90%. Search King handles this by making Expansion **"Optional."** A developer can "Toggle it off" for mobile/web apps where speed is #1, or "Toggle it on" for research portals where "Excellence" is the only metric that matters.

51. **Why is "Min-score Filtering" important for search results?**
    Answer: Min-score is the **"Trash Filter."** Even in a database of 1,000,000 junk files, there will _always_ be a "Top 1 Match," but its "Score" might be 0.1 (meaning it's 10% relevant). If you send that "10% match" to the AI, it will **"Try its best to believe it" (Hallucination).** Min-score says: "If the best match is below 0.5, we have **Zero Knowledge** on this topic." It triggers the "I don't know" fallback. It protects the **"Self-Awareness" of the system.** It is the "Guardrail of Honesty." It ensures that the AI only "Speaks" when it has a "High-Confidence Evidence Base," preventing the "Expert-sounding Lies" that plague lower-tier RAG implementations.

52. **How does SearchKing handle "Empty Queries"?**
    Answer: SearchKing implements **"Internal Short-Circuiting."** If the user query is "Empty" or just "Whitespace," the system returns an **"Empty SearchResult List"** immediately without calling the expensive Embedder or Qdrant. Why? Because an "Empty Embedding" is a **"Mathematical Paradox"**—it points to "Zero Sentiment," which ironically matches the "Most Generic" chunks in your DB. You would retrieve "Random Noise." By "Short-circuiting," we provide **"Logical Integrity."** We treat "Zero Input" as "Zero Intent," resulting in "Zero Context." It ensures the system stays "Silent" until the user provides "Meaning," maintaining a "Professional Boundary" between the computer's logic and the user's input.

53. **What happens if Qdrant is offline?**
    Answer: SearchKing is designed with **"Module Separation."** If the `Qdrant` client fails to connect, or the `vector_store_search` throws a `ConnectionError`, the SearchKing "Catches" the exception and **"Gracefully Fallbacks to BM25 Only."** This is the **"Keyword Emergency Mode."** The user will still get "Literal results." The search will feel "Simpler" (no meaning matches), but the **"Application stays alive."** This "Fault Tolerance" is an absolute requirement for "Live Production Systems." It ensures that "Search Continuity" is preserved even during database maintenance or network blips, providing a "Reliable and Robust" experience that users can trust during "High-Stakes" operations.

54. **Explain the `final_score` calculation logic.**
    Answer: The `final_score` is the **"Weighted Convergence of three dimensions."** (1) **Semantic Match**: How close is the "Deep Meaning"? (2) **Keyword Match**: How many exact words overlap? (3) **Reranker Match**: How well does the chunk answer the specific question? We merge these into a single percent: `0.7 * Semantics + 0.3 * Keywords`. Then we "Multiply" by the Reranker's confidence. This **"Tiered Logic"** ensures that the final result is a **"Logical Consensus."** It turns "Raw Metric Space" into a **"Human probability of success."** It allows the system to be "Scientific" in its internal math but "Intuitive" in its final output, resulting in a ranking that human users almost always agree with.

55. **Why use 3072 dimensions for embeddings?**
    Answer: 3072 is the **"Maximum Semantic Bandwidth"** of the `text-embedding-3-large` model. Dimensions are the **"Brain's Synapses"** for AI. A vector with 3,072 numbers can "Store" 3,072 different "Concepts" about a sentence. It captures "Tone," "Sarcasm," "Technical Precision," and "Logical Flow." Smaller models (like 1536 dims) are "Coarse"—they "Blur" different ideas together. For high-end RAG, we want the **"Full Clarity" of the data.** 3072 ensures that two "Slightly different" philosophical points are mapped to "Distinctly different" points in the database. It is the "Ultra-High Definition" setting for AI search, providing the "Highest Contrast" between "True Context" and "False Context."

56. **What is "Contextual Compression"?**
    Answer: Compression is the **"Information Density Filter."** After the Search King finds the Top 5 chunks, it doesn't just send the whole 4,000 tokens to the AI. It uses a **"Summarizer" or "MMR Filter"** to find the specific "Golden Sentences." It "Deletes the rest." This is **"Prompt Optimization."** It reduces the "Noise" the AI has to ignore. It focuses the AI's "Attention" on exactly 3 sentences of truth. Contextual Compression leads to **"Sharper, Faster Answers"** and saves 30-50% in "API Token Costs." It's the "Efficiency layer" that turns "Big Data" into **"Actionable Intelligence,"** ensuring that every penny spent on the AI is centered on the absolute "Core Evidence" provided by the documents.

57. **Explain the benefits of "Local First" architecture.**
    Answer: Local-First (Local PDF + Local Index + Local Reranker) is the **"Privacy Sovereign" Model.** It ensures that **"The Knowledge Never Leaves your machine."** While we use OpenAI for math (Embeddings), the "Decision of what to find" (Search King) and the "Data Storage" (Qdrant) are local. This provides **"Privacy Compliance"** (e.g., GDPR) for companies who can't upload sensitive data to a 3rd party search engine. It also provides **"Latency Speed"**—searching a local SSD is 10x faster than a network API. Local-First is the "Strategic Choice" for high-security environments where "Data Ownership" is the #1 priority, and it's why Search King is designed to run "Deeply Local."

58. **How would you incorporate "User Feedback" into this ranking?**
    Answer: I would use **"Hybrid Feedback Bias."** When a user "Thumbs Up" an answer, I "Tag" the contributing chunks in the database with `plus_points = 1`. In the Search King's next search, I define a **"Loyalty Weight."** I add a bonus to any chunk that was part of a "Successful past answer." This is **"Reinforcement through Retrieval."** The system "Learns" what facts are most helpful. It turns the Search King from a "Static List" into an **"Evolving Memory."** Over time, the system will "Favor" the most "Practical Truths" in its collection, naturally "Self-Tuning" its ranking logic to perfectly match the "Real-world Success" of its users' inquiries.

59. **Why is the " Krishnamurti" teacher name injected into expansion?**
    Answer: Injection of **"Domain Anchors"** is a **"Contextual grounding" strategy.** In a philosophy database, a word like "Silence" can mean 100 different things. But in _this_ project, it has a "Specific Definition" used by the author (Krishnamurti). By "Adding the Author's Name" to the expanded query, we **"Constraint the AI Universe."** We are "Informing" the search: "Look for the definition of silence _specifically in the context of this teacher's logic_." It prevents the search engine from finding "Random Dictionary Definitions." It ensures **"Semantic Integrity."** It forces the Search King to stay "On-Topic," resulting in results that are "Deeply Brand-Aware" and logically consistent with the specific library being indexed.

60. **Design a "SearchKing" for a Medical Diagnosis system.**
    Answer: (1) **Expansion**: Add "Synonyms" for symptoms (e.g., "Headache" -> "Migraine", "Cephalalgia"). (2) **RRF Weights**: Use **0.9 BM25 / 0.1 Vector**. Why? because in medicine, a "Drug Name" is an exact keyword that _must_ match perfectly. (3) **Neighbor Window**: Set to `window=3`. You need the "Symptoms," the "Treatment," and the "Warning" which are usually scattered across 3-4 chunks. (4) **Filtering**: Add a `Filter: source == "Peer_Reviewed"`. This design creates a **"Safety-First Medical Retriever."** It prioritizes "Literal keyword precision" and "Broad contextual safety" over "Fuzzy meaning," ensuring that the doctor receives the most **"Accurate, Verifiable, and Multi-layered Evidence"** for their diagnosis.
