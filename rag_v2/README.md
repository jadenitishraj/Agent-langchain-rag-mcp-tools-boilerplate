# RAG v2 - Production Grade

A production-ready Retrieval-Augmented Generation system with enterprise features.

## ğŸš€ Features

| Feature | Implementation |
|---------|----------------|
| **Vector Database** | Qdrant (local file-based, no server needed) |
| **Chunking** | Semantic chunking with smart sentence grouping |
| **Embeddings** | OpenAI text-embedding-3-large (3072 dims) |
| **Search** | Hybrid (Vector + BM25 keyword search) |
| **Reranking** | Relevance scoring + diversity (MMR-style) |
| **Storage** | INT8 quantization (4x smaller) |
| **Evaluation** | RAGAS-inspired metrics |

## ğŸ“ Structure

```
rag_v2/
â”œâ”€â”€ __init__.py       # Module exports
â”œâ”€â”€ config.py         # All configuration settings
â”œâ”€â”€ loader.py         # Document loading (PDF, TXT, MD)
â”œâ”€â”€ chunker.py        # Semantic chunking
â”œâ”€â”€ embedder.py       # OpenAI embeddings with batching
â”œâ”€â”€ vector_store.py   # Qdrant vector database
â”œâ”€â”€ bm25_index.py     # BM25 keyword index
â”œâ”€â”€ reranker.py       # Result reranking + diversity
â”œâ”€â”€ query.py          # Hybrid search engine
â”œâ”€â”€ pipeline.py       # Indexing orchestrator
â”œâ”€â”€ evaluation.py     # RAGAS-style evaluation
â””â”€â”€ documents/        # Put your documents here
```

## ğŸ› ï¸ Usage

### 1. Build the Index

```bash
# From project root
./venv/bin/python -m rag_v2.pipeline --build
```

### 2. Test Search

```bash
./venv/bin/python -m rag_v2.pipeline --test
```

### 3. Interactive Mode

```bash
./venv/bin/python -m rag_v2.pipeline --interactive
```

### 4. Single Query

```bash
./venv/bin/python -m rag_v2.pipeline --query "What is meditation?"
```

### 5. Run Evaluation

```bash
./venv/bin/python -m rag_v2.evaluation
```

## âš™ï¸ Configuration

Edit `config.py` to customize:

```python
# Chunking
CHUNK_MIN_SIZE = 100        # Min chars per chunk
CHUNK_MAX_SIZE = 1500       # Max chars per chunk

# Search
VECTOR_WEIGHT = 0.7         # Vector similarity weight
BM25_WEIGHT = 0.3           # Keyword match weight
DEFAULT_TOP_K = 5           # Initial retrieval count
RERANK_TOP_K = 3            # Final count after reranking

# Storage
ENABLE_QUANTIZATION = True  # INT8 quantization
```

## ğŸ”„ How Hybrid Search Works

```
User Query
    â”‚
    â”œâ”€â†’ Vector Search (semantic similarity)
    â”‚   â””â”€â†’ Embeds query, finds similar vectors in Qdrant
    â”‚
    â”œâ”€â†’ BM25 Search (keyword matching)
    â”‚   â””â”€â†’ Tokenizes query, matches against BM25 index
    â”‚
    â””â”€â†’ Combine Results (Reciprocal Rank Fusion)
        â”‚
        â””â”€â†’ Rerank (relevance scoring)
            â”‚
            â””â”€â†’ Diversity Filter (remove redundancy)
                â”‚
                â””â”€â†’ Final Top-K Results
```

## ğŸ“Š Evaluation Metrics

1. **Context Relevancy**: How relevant is retrieved context to the question?
2. **Answer Relevancy**: Does the answer address the question?
3. **Faithfulness**: Is the answer grounded in the context?
4. **Keyword Score**: Are expected keywords found in context?

## ğŸ†š v1 vs v2 Comparison

| Feature | RAG v1 | RAG v2 |
|---------|--------|--------|
| Vector DB | JSON file | Qdrant |
| Chunking | Token-based | Semantic |
| Search | Vector only | Hybrid (Vector + BM25) |
| Reranking | None | Yes |
| Diversity | None | MMR-style |
| Quantization | None | INT8 (4x smaller) |
| Evaluation | Manual | RAGAS-style metrics |
| Multi-format | PDF only | PDF, TXT, MD |

## ğŸ¯ Production Checklist

- [x] Persistent vector storage (Qdrant)
- [x] Hybrid search (semantic + keyword)
- [x] Result reranking
- [x] Diversity filtering
- [x] Storage optimization (quantization)
- [x] Evaluation metrics
- [x] CLI interface
- [x] Error handling
- [x] Logging
- [ ] API rate limiting
- [ ] Caching layer
- [ ] Monitoring/observability
