{
  "summary": {
    "faithfulness": 0.962962962962963,
    "answer_relevancy": 0.9287955591362111,
    "context_precision": 0.9166666666291666,
    "context_recall": 1.0
  },
  "details": [
    {
      "user_input": "What is the purpose of the chunks in this system?",
      "retrieved_contexts": [
        "## File: rag_v2/chunker.py\n\n**Description:** Semantic Chunking - Groups semantically related sentences together Falls back to token-based chunking if needed\n\n**Path:** `rag_v2/chunker.py`\n\n```py\n\"\"\"\nSemantic Chunking - Groups semantically related sentences together\nFalls back to token-based chunking if needed\n\"\"\"\nimport re\nfrom typing import List, Dict\nfrom dataclasses import dataclass\n\nfrom .config import (\n    CHUNK_MIN_SIZE, \n    CHUNK_MAX_SIZE, \n    SIMILARITY_THRESHOLD,\n    FALLBACK_CHUNK_SIZE,\n    FALLBACK_CHUNK_OVERLAP\n)\n\n@dataclass\nclass Chunk:\n    \"\"\"Represents a text chunk with metadata.\"\"\"\n    text: str\n    metadata: Dict\n    chunk_index: int\n\ndef split_into_sentences(text: str) -> List[str]:\n    \"\"\"Split text into sentences using regex.\"\"\"\n    # Clean the text first\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split on sentence boundaries\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    \n    # Filter out empty sentences\n    sentences = [s.strip() for s in sentences if s.strip()]\n    \n    return sentences\n\ndef clean_text(text: str) -> str:\n    \"\"\"Clean and normalize text.\"\"\"\n    # Remove timestamps\n    text = re.sub(r'\\b\\d+:\\d+\\b', '', text)\n    # Remove page numbers (standalone numbers on lines)\n    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ndef simple_semantic_chunk(sentences: List[str], embedder_fn=None) -> List[str]:\n    \"\"\"\n    Create semantic chunks by grouping text.\n    Handles both proper sentences and transcripts without punctuation.\n    \"\"\"\n    if not sentences:\n        return []\n    \n    # Join all sentences back together\n    full_text = ' '.join(sentences)\n    \n    # If no proper sentence endings, just split by size\n    has_punctuation = any(full_text.count(p) > 3 for p in ['.', '!', '?'])\n    \n    if not has_punctuation:\n        # Split by newlines/paragraphs first, then by size\n        paragraphs = full_text.split('\\n')\n        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n        \n        chunks = []\n        current_chunk = []\n        current_size = 0\n        \n        for para in paragraphs:\n            if current_size + len(para) > CHUNK_MAX_SIZE and current_chunk:\n                chunks.append(' '.join(current_chunk))\n                current_chunk = []\n                current_size = 0\n            \n            current_chunk.append(para)\n            current_size += len(para)\n            \n            # Force split at 60% to create more chunks\n            if current_size >= CHUNK_MAX_SIZE * 0.6:\n                chunks.append(' '.join(current_chunk))\n                current_chunk = []\n                current_size = 0\n        \n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        \n        return chunks\n    \n    # Original logic for properly punctuated text\n    chunks = []\n    current_chunk = []\n    current_size = 0\n    \n    for sentence in sentences:\n        sentence_len = len(sentence)\n        \n        if current_size + sentence_len > CHUNK_MAX_SIZE and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = []\n            current_size = 0\n        \n        current_chunk.append(sentence)\n        current_size += sentence_len + 1\n        \n        if current_size >= CHUNK_MAX_SIZE * 0.6 and sentence.endswith(('.', '!', '?')):\n            chunks.append(' '.join(current_chunk))\n            current_chunk = []\n            current_size = 0\n    \n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n\ndef merge_small_chunks(chunks: List[str]) -> List[str]:\n    \"\"\"Merge chunks that are too small.\"\"\"\n    if not chunks:\n        return []\n    \n    merged = []\n    current = chunks[0]\n    \n    for chunk in chunks[1:]:\n        if len(current) < CHUNK_MIN_SIZE:\n            current += ' ' + chunk\n        else:\n            merged.append(current)\n            current = chunk\n    \n    merged.append(current)\n    return merged\n\ndef chunk_document(content: str, source: str, embedder_fn=None) -> List[Chunk]:\n    \"\"\"\n    Chunk a document using size-based strategy.\n    For transcripts without punctuation, splits by character count.\n    \"\"\"\n    # Remove timestamps first\n    content = re.sub(r'\\b\\d+:\\d+\\b', '', content)\n    \n    # Check if text has proper punctuation\n    has_punctuation = any(content.count(p) > 5 for p in ['.', '!', '?'])\n    \n    if has_punctuation:\n        # Use sentence-based chunking\n        cleaned = clean_text(content)\n        sentences = split_into_sentences(cleaned)\n        raw_chunks = simple_semantic_chunk(sentences, embedder_fn)\n    else:\n        # For transcripts: split by lines, then group by size\n        lines = content.split('\\n')\n        lines = [l.strip() for l in lines if l.strip()]\n        \n        raw_chunks = []\n        current_chunk = []\n        current_size = 0\n        \n        for line in lines:\n            line_len = len(line)\n            \n            if current_size + line_len > CHUNK_MAX_SIZE and current_chunk:\n                chunk_text = ' '.join(current_chunk)\n                chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()  # Clean\n                raw_chunks.append(chunk_text)\n                current_chunk = []\n                current_size = 0\n            \n            current_chunk.append(line)\n            current_size += line_len\n            \n            # Force split at target size\n            if current_size >= CHUNK_MAX_SIZE * 0.6:\n                chunk_text = ' '.join(current_chunk)\n                chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()\n                raw_chunks.append(chunk_text)\n                current_chunk = []\n                current_size = 0\n        \n        if current_chunk:\n            chunk_text = ' '.join(current_chunk)\n            chunk_text = re.sub(r'\\s+', ' ', chunk_text).strip()\n            raw_chunks.append(chunk_text)\n    \n    # Merge small chunks\n    final_chunks = merge_small_chunks(raw_chunks)\n    \n    # Convert to Chunk objects\n    chunks = []\n    for i, text in enumerate(final_chunks):\n        if text and len(text) > 50:  # Skip tiny chunks\n            chunks.append(Chunk(\n                text=text,\n                metadata={\n                    \"source\": source,\n                    \"chunk_index\": i,\n                    \"total_chunks\": len(final_chunks),\n                    \"char_count\": len(text)\n                },\n                chunk_index=i\n            ))\n    \n    return chunks\n\ndef chunk_documents(documents: List, embedder_fn=None) -> List[Chunk]:\n    \"\"\"\n    Chunk multiple documents.\n    \n    Args:\n        documents: List of Document objects\n        embedder_fn: Optional embedding function\n    \n    Returns:\n        List of all chunks from all documents\n    \"\"\"\n    all_chunks = []\n    \n    for doc in documents:\n        print(f\"   Chunking: {doc.metadata.get('filename', 'unknown')}\")\n        chunks = chunk_document(doc.content, doc.source, embedder_fn)\n        all_chunks.extend(chunks)\n        print(f\"   \u2713 Created {len(chunks)} chunks\")\n    \n    print(f\"\\n   Total chunks: {len(all_chunks)}\")\n    return all_chunks\n\n```\n",
        "## File: ARCHITECTURE.md\n\n**Description:** Source file in Agent-langchain-rag-mcp-tools-boilerplate/\n\n**Path:** `ARCHITECTURE.md`\n\n```md\n# System Architecture \ud83c\udfd7\ufe0f\n\nThis document describes the high-level architecture of the AgentForge boilerplate.\n\n## \ud83e\udde9 Core Components\n\n### 1. Agent Logic (`langraph/`)\nThe core of the system is built on **LangGraph**, a stateful orchestration library for LLM agents.\n- **State Graph**: Defines the flow (nodes and edges) of the agent.\n- **Nodes**:\n  - `validate_input`: Checks user input for safety.\n  - `retrieve`: Fetches relevant context from RAG.\n  - `call_model`: Invokes the LLM (GPT-4o) with tools.\n  - `execute_tools`: Runs requested tools.\n  - `validate_output`: Sanitizes the final response.\n- **Memory**: Persistent conversation history and user memories.\n\n### 2. RAG Pipeline (`rag/` & `rag_v2/`)\nRetrieval Augmented Generation allows the agent to access custom knowledge (like this codebase).\n- **Ingestion**: `loader.py` reads files (PDFs, code, markdown).\n- **Chunking**: `processor.py` splits text into semantic chunks.\n- **Embedding**: `embedder.py` converts text to vectors (OpenAI Ada-002/3).\n- **Storage**: Local JSON/NumPy vector store (ChromaDB compatible).\n- **Retrieval**: Hybrid search (Vector + Keyword) with re-ranking.\n\n### 3. Model Context Protocol (`mcp_servers/`)\nImplements Anthropic's [MCP](https://modelcontextprotocol.io/) to standardize tool connections.\n- **MCP Client**: Connects the LangGraph agent to MCP servers.\n- **MCP Servers**:\n  - `duckduckgo_server`: Web search.\n  - `sqlite_server`: Database operations.\n  - `brave_server`: Alternative search.\n\n### 4. Guardrails (`guardrails/`)\nensure safety and quality at both input and output stages.\n- **Input Guards**:\n  - Prompt Injection Detection\n  - Toxicity/Hate Speech Check\n  - Topic Relevance Filter\n- **Output Guards**:\n  - PII Redaction (Email, Phone, Credit Card)\n  - Hallucination Detection\n  - Response Quality Check\n\n### 5. Frontend (`frontend/`)\nA modern, responsive UI built with:\n- **React 19**: Component-based UI.\n- **Vite**: Fast build tool.\n- **TailwindCSS**: Utility-first styling.\n- **Server-Sent Events (SSE)**: For real-time streaming responses.\n\n## \ud83d\udd04 Data Flow\n\n1. **User Request**: User sends a message via Frontend.\n2. **API Layer**: FastAPI receives request at `/agent/ask/stream`.\n3. **Agent Start**: LangGraph initializes state with history.\n4. **Input Guard**: Validates request. If safe, proceeds.\n5. **Retrieval**: RAG system searches for relevant context.\n6. **Thinking**: LLM processes context + query + tools.\n7. **Tool Execution** (Optional): If tools are called (e.g., search, memory), they execute and return results.\n8. **Generation**: LLM generates final response.\n9. **Output Guard**: Response is checked for PII/safety.\n10. **Streaming**: Response is streamed token-by-token to Frontend.\n\n## \ud83d\uddc4\ufe0f Database Schema\n\nWe use **SQLite** (via SQLAlchemy) for persistent data:\n\n**Users Table**\n- `id`: Integer PK\n- `email`: String (Unique)\n- `hashed_password`: String\n- `is_active`: Boolean\n\n*Note: Agent memories are stored via the SQLite MCP server in a separate structure.*\n\n```\n",
        "## File: rag_v2/documents/PROJECT_GUIDE.md\n\n**Description:** Source file in documents/\n\n**Path:** `rag_v2/documents/PROJECT_GUIDE.md`\n\n```md\n# AgentForge Project Guide & Architecture Explanation \ud83d\udcd8\n\n## \ud83c\udf1f Executive Summary\n\n**AgentForge** is a modern, production-grade boilerplate for building AI Agents. It is not just a chatbot, but a sophisticated **Cognitive Architecture** that combines:\n\n1.  **Reasoning**: Using **LangGraph** to model complex workflows as a graph (nodes & edges).\n2.  **Knowledge**: Using **RAG (Retrieval Augmented Generation)** to ground answers in real data.\n3.  **Action**: Using **MCP (Model Context Protocol)** to safely interact with the outside world (databases, web, APIs).\n4.  **Safety**: Using **Guardrails** to ensure the AI behaves correctly.\n\n---\n\n## \ud83c\udfd7\ufe0f System Architecture: The \"Brain\" Anatomy\n\nImagine the agent as a digital employee. Here is how its \"brain\" is structured:\n\n### 1. The Cortex (LangGraph Agent)\n*Folder: `langraph/`*\n\nThis is the decision-making center. Unlike simple chains (Step A -> Step B), our agent works like a flow chart:\n*   **Router (`router.py`)**: The \"ears\" of the agent. It receives queries from the user via the API.\n*   **Graph (`agent.py`)**: The \"mind\". It decides:\n    *   *\"Should I search memory?\"*\n    *   *\"Should I look up the code?\"*\n    *   *\"Is this a dangerous question?\"*\n*   **State**: It remembers the conversation history and what it's currently working on.\n\n### 2. The Library (RAG Pipeline)\n*Folder: `rag_v2/`*\n\nThe agent doesn't know everything. It uses RAG to \"look up\" information from your documents/codebase.\n*   **Ingestion**: Reading files (PDFs, Code).\n*   **Chunking**: Breaking text into small, meaningful pieces.\n*   **Embedding**: Converting text into numbers (vectors) so computers can compare concepts.\n*   **Vector Database (Qdrant)**: A specialized database that stores these meaning-vectors for fast searching.\n\n**Why v2?** Our v2 pipeline uses \"Semantic Chunking\" \u2013 it keeps related sentences together rather than just cutting text at random spots. This makes retrieval much smarter.\n\n### 3. The Hands (MCP & Tools)\n*Folder: `mcp_servers/` and `tools/`*\n\nAn AI model is just text-in, text-out. To *do* things, it uses Tools.\n*   **MCP (Model Context Protocol)**: A standard way to connect AI to systems. Think of it like \"USB for AI\".\n    *   **DuckDuckGo Server**: Allows the agent to search the web for real-time info.\n    *   **SQLite Server**: persistent memory storage (remembering user preferences).\n*   **Local Tools**: Simple Python functions (e.g., specific math or logic) the agent can run directly.\n\n### 4. The Filters (Guardrails)\n*Folder: `guardrails/`*\n\nThese are the safety checks.\n*   **Input Guard**: Checks the user's question. *\"Is this asking for illegal stuff? Is it prompt injection?\"*\n*   **Output Guard**: Checks the agent's answer. *\"Did I leak a password? Am I hallucinating facts?\"*\n\n### 5. The Face (Frontend)\n*Folder: `frontend/`*\n\nA modern **React 19** interface.\n*   **Streaming**: It shows the answer being typed out in real-time (like ChatGPT).\n*   **Markdown Rendering**: It displays code blocks beautifully with syntax highlighting.\n*   **TailwindCSS**: Professional styling.\n\n---\n\n## \ud83d\udcc2 detailed File Map (English Explanations)\n\n### Root Directory\n*   `main.py`: The **Entry Point**. It starts the web server (FastAPI). It connects the Frontend asking questions to the Backend agent.\n*   `models.py` & `database.py`: Handles user accounts (login/signup) using a traditional SQL database.\n\n### langraph/\n*   `agent.py`: **The Master Controller**. Defines the StateGraph. It says: \"First Validate Input -> Then Retrieval -> Then Call LLM -> Then Validate Output\".\n*   `router.py`: **The API Endpoints**. Defines URLs like `/agent/ask` that the frontend sends data to.\n\n### rag_v2/\n*   `pipeline.py`: **The Coordinator**. Runs the full flow: Read File -> Chunk -> Embed -> Store.\n*   `chunker.py`: **The Scissors**. Smartly cuts text.\n*   `embedder.py`: **The Translator**. Turns text into vector arrays (lists of numbers).\n*   `vector_store.py`: **The Librarian**. Manages storing and finding vectors in Qdrant.\n\n### mcp_servers/\n*   `mcp_client.py`: The bridge that lets our Python Agent talk to MCP servers.\n*   `duckduckgo_server.py`: A mini-server that performs web searches.\n\n### guardrails/\n*   `input_guards.py`: Contains logic to detect \"Jailbreaks\" (users trying to trick the AI).\n*   `output_guards.py`: Contains logic to redact emails/phones (PII) from answers.\n\n---\n\n## \ud83d\udd04 How a Request Flows (The Life of a Message)\n\n1.  **User** types: *\"How does RAG work?\"* in the Frontend.\n2.  **Frontend** sends this text to `main.py` (Backend).\n3.  **Backend** passes it to the `Agent` (`langraph/agent.py`).\n4.  **Agent Step 1 (Guard)**: Checks if the question is safe.\n5.  **Agent Step 2 (Retrieve)**:\n    *   Calls `rag_v2` to look up \"RAG\" in the vector database.\n    *   Finds this very file (`PROJECT_GUIDE.md`) and code files.\n6.  **Agent Step 3 (Think)**:\n    *   Sends the User's question + The Retrieved File Content to GPT-4o.\n    *   System Prompt says: *\"You are an expert developer. Explain using the context.\"*\n7.  **Agent Step 4 (Generate)**: GPT-4o generates the explanation.\n8.  **Agent Step 5 (Guard)**: Checks the answer for secrets/errors.\n9.  **Frontend**: Receives the text stream and displays it nicely.\n\n---\n\n## \ud83d\udd2e Core Concepts Glossary\n\n*   **LangChain**: A library that creates \"chains\" of logic (A -> B -> C).\n*   **LangGraph**: An evolution of LangChain that creates \"graphs\" (loops, branches, states). Essential for complex agents.\n*   **Vector**: A long list of numbers representing meaning. \"King\" and \"Queen\" -> vectors are close. \"King\" and \"Banana\" -> vectors are far apart.\n*   **FastAPI**: A super-fast Python framework for building web APIs (the backend server).\n*   **React**: A Javascript library for building user interfaces (the frontend).\n\n```\n"
      ],
      "response": "In the context of the AgentForge system, chunks serve a crucial role in the **Retrieval Augmented Generation (RAG)** pipeline. Here's a breakdown of their purpose:\n\n1. **Semantic Chunking**: Chunks are used to break down larger documents or text into smaller, semantically related pieces. This is important because it allows the system to retrieve relevant information more efficiently. Instead of searching through large bodies of text, the system can focus on smaller sections that are more likely to contain the relevant information needed to respond to a user's query.\n\n2. **Improved Retrieval**: By chunking text, the system can use vector embeddings to represent the meaning of the text more effectively. Each chunk can be converted into a vector, allowing for similarity searches within a vector database. This enables the agent to retrieve the most relevant chunks of information when responding to user queries.\n\n3. **Contextual Relevance**: Chunks help maintain the context of the information. By grouping semantically related sentences or paragraphs together, the system ensures that the retrieved information is not just relevant in isolation but also maintains coherence and context when being used to generate responses.\n\n4. **Efficiency**: Smaller chunks allow for faster processing and retrieval times. The system can quickly determine which chunks are relevant to a given query without having to analyze entire documents. This efficiency is particularly important for real-time applications where speed is crucial.\n\n5. **Handling Different Types of Content**: The chunking process can adapt to various types of content (e.g., punctuated text vs. transcripts without punctuation). This flexibility ensures that the system can effectively process different sources of information.\n\nIn summary, chunks play a pivotal role in enhancing the effectiveness of the RAG system by improving retrieval accuracy, maintaining contextual relevance, and allowing for efficient processing of information. The chunking logic is implemented in the `chunker.py` file within the `rag_v2` directory, where various functions are defined to handle the chunking process.",
      "reference": "The chunks are semantic segments of documents used for embedding and retrieval in the RAG system.",
      "faithfulness": 1.0,
      "answer_relevancy": 0.8746624240128965,
      "context_precision": 0.9999999999666667,
      "context_recall": 1.0
    },
    {
      "user_input": "How does hybrid search work?",
      "retrieved_contexts": [
        "## File: rag_v2/query.py\n\n**Description:** Query Engine - Hybrid search with reranking Combines vector search (semantic) with BM25 (keyword) for best results\n\n**Path:** `rag_v2/query.py`\n\n```py\n\"\"\"\nQuery Engine - Hybrid search with reranking\nCombines vector search (semantic) with BM25 (keyword) for best results\n\"\"\"\nfrom typing import List, Dict, Optional\n\nfrom .config import (\n    VECTOR_WEIGHT,\n    BM25_WEIGHT,\n    DEFAULT_TOP_K,\n    RERANK_TOP_K\n)\nfrom .embedder import embed_text\nfrom .vector_store import search_vectors, get_all_texts\nfrom .bm25_index import search_bm25, get_text_by_index, load_bm25_index\nfrom .reranker import rerank_results, diversity_rerank\n\ndef normalize_scores(scores: List[float]) -> List[float]:\n    \"\"\"Normalize scores to 0-1 range.\"\"\"\n    if not scores:\n        return []\n    \n    min_score = min(scores)\n    max_score = max(scores)\n    \n    if max_score == min_score:\n        return [1.0] * len(scores)\n    \n    return [(s - min_score) / (max_score - min_score) for s in scores]\n\ndef hybrid_search(\n    query: str,\n    top_k: int = DEFAULT_TOP_K,\n    vector_weight: float = VECTOR_WEIGHT,\n    bm25_weight: float = BM25_WEIGHT,\n    use_reranker: bool = True,\n    use_diversity: bool = True\n) -> List[Dict]:\n    \"\"\"\n    Perform hybrid search combining vector and BM25 results.\n    \n    Args:\n        query: Search query\n        top_k: Number of results to return\n        vector_weight: Weight for vector search (0-1)\n        bm25_weight: Weight for BM25 search (0-1)\n        use_reranker: Whether to apply reranking\n        use_diversity: Whether to apply diversity filter\n    \n    Returns:\n        List of search results\n    \"\"\"\n    # Get more results initially for reranking\n    initial_k = top_k * 3 if use_reranker else top_k\n    \n    # 1. Vector search\n    print(f\"\\n\ud83d\udd0d Hybrid Search for: '{query}'\")\n    print(\"-\" * 50)\n    \n    query_embedding = embed_text(query)\n    vector_results = search_vectors(query_embedding, limit=initial_k)\n    print(f\"   Vector search: {len(vector_results)} results\")\n    \n    # 2. BM25 search\n    bm25_results = search_bm25(query, top_k=initial_k)\n    print(f\"   BM25 search: {len(bm25_results)} results\")\n    \n    # 3. Combine results using Reciprocal Rank Fusion (RRF)\n    text_scores = {}  # text -> (vector_score, bm25_score, combined_score)\n    text_metadata = {}  # text -> metadata\n    \n    # Process vector results\n    vector_scores = [r[\"score\"] for r in vector_results]\n    normalized_vector = normalize_scores(vector_scores)\n    \n    for i, result in enumerate(vector_results):\n        text = result[\"text\"]\n        score = normalized_vector[i] if normalized_vector else 0\n        text_scores[text] = {\"vector\": score, \"bm25\": 0, \"combined\": 0}\n        text_metadata[text] = {\n            \"source\": result.get(\"source\", \"\"),\n            \"metadata\": result.get(\"metadata\", {})\n        }\n    \n    # Process BM25 results\n    bm25_scores_raw = [score for _, score in bm25_results]\n    normalized_bm25 = normalize_scores(bm25_scores_raw)\n    \n    for i, (idx, _) in enumerate(bm25_results):\n        text = get_text_by_index(idx)\n        if text:\n            score = normalized_bm25[i] if normalized_bm25 else 0\n            if text in text_scores:\n                text_scores[text][\"bm25\"] = score\n            else:\n                text_scores[text] = {\"vector\": 0, \"bm25\": score, \"combined\": 0}\n                text_metadata[text] = {\"source\": \"\", \"metadata\": {}}\n    \n    # Calculate combined scores\n    for text in text_scores:\n        vs = text_scores[text][\"vector\"]\n        bs = text_scores[text][\"bm25\"]\n        text_scores[text][\"combined\"] = (vs * vector_weight) + (bs * bm25_weight)\n    \n    # Sort by combined score\n    sorted_texts = sorted(\n        text_scores.keys(),\n        key=lambda t: text_scores[t][\"combined\"],\n        reverse=True\n    )\n    \n    # Build results\n    results = []\n    for text in sorted_texts[:initial_k]:\n        results.append({\n            \"text\": text,\n            \"score\": text_scores[text][\"combined\"],\n            \"vector_score\": text_scores[text][\"vector\"],\n            \"bm25_score\": text_scores[text][\"bm25\"],\n            **text_metadata.get(text, {})\n        })\n    \n    print(f\"   Hybrid combined: {len(results)} results\")\n    \n    # 4. Rerank\n    if use_reranker and results:\n        results = rerank_results(query, results, top_k=top_k * 2)\n        print(f\"   After reranking: {len(results)} results\")\n    \n    # 5. Diversity filter\n    if use_diversity and results:\n        results = diversity_rerank(results, top_k=top_k)\n        print(f\"   After diversity filter: {len(results)} results\")\n    \n    print(\"-\" * 50)\n    \n    return results[:top_k]\n\ndef query_memory(\n    question: str,\n    k: int = RERANK_TOP_K\n) -> List[Dict]:\n    \"\"\"\n    Query the knowledge base.\n    \n    Args:\n        question: The question to search for\n        k: Number of results\n    \n    Returns:\n        List of relevant chunks\n    \"\"\"\n    return hybrid_search(question, top_k=k)\n\ndef get_context(\n    question: str,\n    k: int = RERANK_TOP_K\n) -> str:\n    \"\"\"\n    Get context string for RAG prompt.\n    \n    Args:\n        question: The question\n        k: Number of chunks to include\n    \n    Returns:\n        Formatted context string\n    \"\"\"\n    results = query_memory(question, k=k)\n    \n    if not results:\n        return \"\"\n    \n    # Format context with source attribution\n    context_parts = []\n    for i, result in enumerate(results, 1):\n        source = result.get(\"source\", \"Unknown\")\n        text = result.get(\"text\", \"\")\n        score = result.get(\"score\", 0)\n        \n        context_parts.append(f\"[Source {i}: {source} (relevance: {score:.2f})]\\n{text}\")\n    \n    return \"\\n\\n---\\n\\n\".join(context_parts)\n\ndef simple_search(query: str, k: int = 5) -> List[Dict]:\n    \"\"\"\n    Simple vector-only search (for comparison/debugging).\n    \"\"\"\n    query_embedding = embed_text(query)\n    return search_vectors(query_embedding, limit=k)\n\n```\n",
        "## File: rag_v2/bm25_index.py\n\n**Description:** BM25 Index - Keyword-based search for hybrid retrieval\n\n**Path:** `rag_v2/bm25_index.py`\n\n```py\n\"\"\"\nBM25 Index - Keyword-based search for hybrid retrieval\n\"\"\"\nimport os\nimport json\nimport pickle\nfrom typing import List, Tuple, Optional\nfrom rank_bm25 import BM25Okapi\n\nfrom .config import QDRANT_PATH\n\nBM25_INDEX_PATH = os.path.join(QDRANT_PATH, \"bm25_index.pkl\")\nBM25_CORPUS_PATH = os.path.join(QDRANT_PATH, \"bm25_corpus.json\")\n\n# Global BM25 index\n_bm25_index: Optional[BM25Okapi] = None\n_corpus: Optional[List[str]] = None\n\ndef tokenize(text: str) -> List[str]:\n    \"\"\"Simple tokenization for BM25.\"\"\"\n    # Lowercase and split on non-alphanumeric\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Remove very short tokens\n    tokens = [t for t in tokens if len(t) > 2]\n    \n    return tokens\n\ndef build_bm25_index(texts: List[str]) -> BM25Okapi:\n    \"\"\"\n    Build a BM25 index from texts.\n    \n    Args:\n        texts: List of document texts\n    \n    Returns:\n        BM25Okapi index\n    \"\"\"\n    global _bm25_index, _corpus\n    \n    print(\"   Building BM25 index...\")\n    \n    # Tokenize all texts\n    tokenized_corpus = [tokenize(text) for text in texts]\n    \n    # Create BM25 index\n    _bm25_index = BM25Okapi(tokenized_corpus)\n    _corpus = texts\n    \n    # Save index and corpus\n    os.makedirs(QDRANT_PATH, exist_ok=True)\n    \n    with open(BM25_INDEX_PATH, 'wb') as f:\n        pickle.dump(_bm25_index, f)\n    \n    with open(BM25_CORPUS_PATH, 'w') as f:\n        json.dump(texts, f)\n    \n    print(f\"   \u2713 BM25 index built with {len(texts)} documents\")\n    \n    return _bm25_index\n\ndef load_bm25_index() -> Tuple[Optional[BM25Okapi], Optional[List[str]]]:\n    \"\"\"\n    Load BM25 index from disk.\n    \n    Returns:\n        Tuple of (BM25 index, corpus)\n    \"\"\"\n    global _bm25_index, _corpus\n    \n    if _bm25_index is not None:\n        return _bm25_index, _corpus\n    \n    if not os.path.exists(BM25_INDEX_PATH):\n        return None, None\n    \n    try:\n        with open(BM25_INDEX_PATH, 'rb') as f:\n            _bm25_index = pickle.load(f)\n        \n        with open(BM25_CORPUS_PATH, 'r') as f:\n            _corpus = json.load(f)\n        \n        return _bm25_index, _corpus\n    except Exception as e:\n        print(f\"   \u26a0\ufe0f  Error loading BM25 index: {e}\")\n        return None, None\n\ndef search_bm25(query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n    \"\"\"\n    Search using BM25.\n    \n    Args:\n        query: Search query\n        top_k: Number of results\n    \n    Returns:\n        List of (index, score) tuples\n    \"\"\"\n    index, corpus = load_bm25_index()\n    \n    if index is None:\n        return []\n    \n    # Tokenize query\n    query_tokens = tokenize(query)\n    \n    if not query_tokens:\n        return []\n    \n    # Get scores for all documents\n    scores = index.get_scores(query_tokens)\n    \n    # Get top-k indices\n    import numpy as np\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    \n    results = []\n    for idx in top_indices:\n        if scores[idx] > 0:  # Only include if there's some match\n            results.append((int(idx), float(scores[idx])))\n    \n    return results\n\ndef get_text_by_index(index: int) -> Optional[str]:\n    \"\"\"Get text by corpus index.\"\"\"\n    _, corpus = load_bm25_index()\n    \n    if corpus and 0 <= index < len(corpus):\n        return corpus[index]\n    \n    return None\n\n```\n",
        "## File: rag_v2/pipeline.py\n\n**Description:** RAG v2 Pipeline - Production Grade Orchestrates the full indexing process\n\n**Path:** `rag_v2/pipeline.py`\n\n```py\n#!/usr/bin/env python3\n\"\"\"\nRAG v2 Pipeline - Production Grade\nOrchestrates the full indexing process\n\"\"\"\nimport os\nimport sys\nimport time\n\n# Add parent directory to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom rag_v2.config import PDF_FOLDER, COLLECTION_NAME\nfrom rag_v2.loader import load_documents\nfrom rag_v2.chunker import chunk_documents\nfrom rag_v2.embedder import embed_chunks\nfrom rag_v2.vector_store import store_vectors, get_collection_info\nfrom rag_v2.bm25_index import build_bm25_index\nfrom rag_v2.query import get_context, hybrid_search\n\ndef build_index(recreate: bool = True) -> bool:\n    \"\"\"\n    Build the complete RAG index.\n    \n    Args:\n        recreate: If True, recreate the entire index\n    \n    Returns:\n        True if successful\n    \"\"\"\n    start_time = time.time()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\ude80 RAG v2 PIPELINE - Production Grade\")\n    print(\"=\" * 60)\n    print(f\"   Collection: {COLLECTION_NAME}\")\n    print(f\"   Documents folder: {PDF_FOLDER}\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    # Step 1: Load documents\n    print(\"\ud83d\udcc4 STEP 1: Loading Documents\")\n    print(\"-\" * 40)\n    documents = load_documents(PDF_FOLDER)\n    \n    if not documents:\n        print(\"\\n\u26a0\ufe0f  No documents to process!\")\n        print(f\"   Add files to: {PDF_FOLDER}\")\n        return False\n    \n    print(f\"\\n   \u2713 Loaded {len(documents)} document(s)\\n\")\n    \n    # Step 2: Chunk documents (semantic chunking)\n    print(\"\u2702\ufe0f  STEP 2: Semantic Chunking\")\n    print(\"-\" * 40)\n    chunks = chunk_documents(documents)\n    \n    if not chunks:\n        print(\"\\n\u26a0\ufe0f  No chunks created!\")\n        return False\n    \n    print()\n    \n    # Step 3: Generate embeddings\n    print(\"\ud83e\udde0 STEP 3: Generating Embeddings\")\n    print(\"-\" * 40)\n    embeddings = embed_chunks(chunks)\n    print()\n    \n    # Step 4: Store in Qdrant\n    print(\"\ud83d\udcbe STEP 4: Storing in Qdrant Vector DB\")\n    print(\"-\" * 40)\n    store_vectors(chunks, embeddings, recreate=recreate)\n    print()\n    \n    # Step 5: Build BM25 index for hybrid search\n    print(\"\ud83d\udcda STEP 5: Building BM25 Index\")\n    print(\"-\" * 40)\n    texts = [chunk.text for chunk in chunks]\n    build_bm25_index(texts)\n    print()\n    \n    # Summary\n    elapsed = time.time() - start_time\n    print(\"=\" * 60)\n    print(\"\u2705 INDEXING COMPLETE!\")\n    print(\"=\" * 60)\n    print(f\"   Documents processed: {len(documents)}\")\n    print(f\"   Chunks created: {len(chunks)}\")\n    print(f\"   Vectors stored: {len(embeddings)}\")\n    print(f\"   Time elapsed: {elapsed:.2f}s\")\n    print()\n    \n    # Show collection info\n    info = get_collection_info()\n    print(\"\ud83d\udcca Collection Info:\")\n    print(f\"   Name: {info.get('name', 'N/A')}\")\n    print(f\"   Points: {info.get('points_count', 'N/A')}\")\n    print(f\"   Status: {info.get('status', 'N/A')}\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    return True\n\ndef test_search(query: str = \"What is meditation?\"):\n    \"\"\"Test the search functionality.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83e\uddea TEST SEARCH\")\n    print(\"=\" * 60)\n    \n    results = hybrid_search(query, top_k=3)\n    \n    print(f\"\\n\ud83d\udcdd Query: '{query}'\")\n    print(f\"   Found {len(results)} results\\n\")\n    \n    for i, result in enumerate(results, 1):\n        print(f\"--- Result {i} ---\")\n        print(f\"Score: {result.get('score', 0):.4f}\")\n        print(f\"Source: {result.get('source', 'Unknown')}\")\n        print(f\"Text: {result.get('text', '')[:200]}...\")\n        print()\n\ndef interactive_mode():\n    \"\"\"Interactive query mode.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83d\udd2e INTERACTIVE MODE\")\n    print(\"=\" * 60)\n    print(\"Type your questions (or 'exit' to quit)\\n\")\n    \n    while True:\n        try:\n            query = input(\"You \u2192 \").strip()\n            \n            if query.lower() in ['exit', 'quit', 'q']:\n                print(\"\\n\ud83d\udc4b Goodbye!\")\n                break\n            \n            if not query:\n                continue\n            \n            context = get_context(query, k=3)\n            \n            print(\"\\n--- Retrieved Context ---\")\n            print(context if context else \"No relevant context found.\")\n            print(\"-\" * 40 + \"\\n\")\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\n\ud83d\udc4b Goodbye!\")\n            break\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"RAG v2 Pipeline\")\n    parser.add_argument(\"--build\", action=\"store_true\", help=\"Build the index\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Run test search\")\n    parser.add_argument(\"--interactive\", \"-i\", action=\"store_true\", help=\"Interactive mode\")\n    parser.add_argument(\"--query\", \"-q\", type=str, help=\"Single query\")\n    \n    args = parser.parse_args()\n    \n    if args.build:\n        build_index()\n    elif args.test:\n        test_search()\n    elif args.interactive:\n        interactive_mode()\n    elif args.query:\n        context = get_context(args.query)\n        print(context)\n    else:\n        # Default: build and test\n        if build_index():\n            test_search()\n\n```\n"
      ],
      "response": "Hybrid search combines two different search methods\u2014vector search and BM25 (a keyword-based search algorithm)\u2014to provide more comprehensive and relevant results. This approach is particularly effective in scenarios where both semantic understanding and keyword matching are important.\n\nHere\u2019s how it works, based on the code in `rag_v2/query.py`:\n\n### 1. **Vector Search:**\n   - **Embedding the Query:** The query is first converted into a vector representation using an embedding model (via the `embed_text` function).\n   - **Searching Vectors:** This vector is then used to search through a vector store to retrieve documents that are semantically similar to the query. This is handled by the `search_vectors` function, which returns a list of results based on their vector similarity to the query.\n\n### 2. **BM25 Search:**\n   - **Tokenizing the Query:** The query is tokenized into keywords.\n   - **Searching with BM25:** The BM25 algorithm is then applied to the same query to retrieve a set of documents based on keyword matching. This is done using the `search_bm25` function, which ranks documents based on how well they match the query keywords.\n\n### 3. **Combining Results:**\n   - **Normalizing Scores:** The scores from both searches (vector and BM25) are normalized to a common scale (0-1) using the `normalize_scores` function.\n   - **Reciprocal Rank Fusion (RRF):** The results from both searches are then combined into a unified ranking based on their scores. This allows the system to leverage the strengths of both methods: the semantic understanding from the vector search and the precise keyword matching from BM25.\n\n### 4. **Reranking and Diversity Filtering:**\n   - **Reranking:** If enabled, reranking is applied to the combined results to ensure the most relevant documents are prioritized. This is done using the `rerank_results` function.\n   - **Diversity:** Additionally, a diversity filter may be applied to ensure that the results cover a range of topics or perspectives, thereby enhancing the overall quality of the output.\n\n### Code Reference:\nHere's the main function that performs the hybrid search:\n\n```python\ndef hybrid_search(\n    query: str,\n    top_k: int = DEFAULT_TOP_K,\n    vector_weight: float = VECTOR_WEIGHT,\n    bm25_weight: float = BM25_WEIGHT,\n    use_reranker: bool = True,\n    use_diversity: bool = True\n) -> List[Dict]:\n    # Vector search\n    query_embedding = embed_text(query)\n    vector_results = search_vectors(query_embedding, limit=initial_k)\n    \n    # BM25 search\n    bm25_results = search_bm25(query, top_k=initial_k)\n    \n    # Combine results\n    # Normalization and scoring logic follows...\n```\n\n### Summary:\nBy combining these two methods, hybrid search enhances the quality and relevance of search results, making it particularly suitable for applications involving complex queries where both semantic understanding and keyword matching are crucial.",
      "reference": "Hybrid search combines vector search (semantic) with BM25 (keyword) search and uses Reciprocal Rank Fusion (RRF) to combine the scores.",
      "faithfulness": 0.9259259259259259,
      "answer_relevancy": 0.9829286942595258,
      "context_precision": 0.8333333332916666,
      "context_recall": 1.0
    }
  ]
}